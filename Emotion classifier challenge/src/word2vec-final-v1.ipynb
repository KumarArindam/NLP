{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aman Deep Singh\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Flatten, Embedding, CuDNNLSTM, Bidirectional, concatenate, Dropout, LeakyReLU\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 metrics function\n",
    "def f1(y_true, y_pred):\n",
    "    '''returns the f1 score given targets and predictions'''\n",
    "    \n",
    "    def recall(y_true, y_pred):\n",
    "        true_pos = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_pos = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        rec = true_pos / (possible_pos + K.epsilon())\n",
    "        return rec\n",
    "    \n",
    "    def precision(y_true, y_pred):\n",
    "        true_pos = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_pos = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        prec = true_pos / (predicted_pos + K.epsilon())\n",
    "        return prec\n",
    "    \n",
    "    _precision = precision(y_true, y_pred)\n",
    "    _recall = recall(y_true, y_pred)\n",
    "    return 2 * ((_precision * _recall) / (_precision + _recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60321, 3) 60321\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('D:/Datasets/hackerearth/hm_train.csv')\n",
    "df.drop(['num_sentence'], axis=1, inplace=True)\n",
    "labels = df.predicted_category\n",
    "df.drop(['predicted_category'], axis=1, inplace=True)\n",
    "\n",
    "print(df.shape, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>cleaned_hm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27673</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went on a successful date with someone I fel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27674</td>\n",
       "      <td>24h</td>\n",
       "      <td>I was happy when my son got 90% marks in his e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27675</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went to the gym this morning and did yoga.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27676</td>\n",
       "      <td>24h</td>\n",
       "      <td>We had a serious talk with some friends of our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27677</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went with grandchildren to butterfly display...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid reflection_period                                         cleaned_hm\n",
       "0  27673               24h  I went on a successful date with someone I fel...\n",
       "1  27674               24h  I was happy when my son got 90% marks in his e...\n",
       "2  27675               24h       I went to the gym this morning and did yoga.\n",
       "3  27676               24h  We had a serious talk with some friends of our...\n",
       "4  27677               24h  I went with grandchildren to butterfly display..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cleaned_hm = df.cleaned_hm.str.lower()\n",
    "SEQ_LEN = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence):\n",
    "    # removing some known errors and differences between american and british english\n",
    "    sentence = sentence.replace(\"\\n\", \" \").replace(\";\", \" ; \").replace(\":\", \" : \").replace(\",\", \" , \").replace(\".\", \" . \").replace(\"?\", \" ? \").replace(\"/\", \" / \").replace(\"\\\\\", \" \\ \").replace(\"'s\", \"\").replace(\"n't\", \" not\").replace(\"travelled\", \"traveled\").replace(\"traveller\", \"traveler\").replace(\"cancelled\", \"canceled\").replace(\"favourite\", \"favorite\").replace(\"i'm\", \"i am\").replace(\"i've\", \"i have\").replace(\"colour\", \"color\").replace(\"neighbour\", \"neighbor\").replace(\"jewellery\", \"jewelry\").replace(\"theatre\", \"theater\").replace(\"i'd\", \"i would\").replace(\"didnt\", \"did not\").replace(\"doesnt\", \"does not\").replace(\"wasnt\", \"was not\").replace(\"programme\", \"program\").replace(\"organise\", \"organize\")\n",
    "    \n",
    "    split = sentence.split()\n",
    "    if len(split) > SEQ_LEN:\n",
    "        return ' '.join([w for w in sentence.split() if w not in stopwords.words('english')])\n",
    "    else:\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>cleaned_hm2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27673</td>\n",
       "      <td>24h</td>\n",
       "      <td>i went on a successful date with someone i fel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27674</td>\n",
       "      <td>24h</td>\n",
       "      <td>i was happy when my son got 90% marks in his e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27675</td>\n",
       "      <td>24h</td>\n",
       "      <td>i went to the gym this morning and did yoga .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27676</td>\n",
       "      <td>24h</td>\n",
       "      <td>we had a serious talk with some friends of our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27677</td>\n",
       "      <td>24h</td>\n",
       "      <td>i went with grandchildren to butterfly display...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid reflection_period                                        cleaned_hm2\n",
       "0  27673               24h  i went on a successful date with someone i fel...\n",
       "1  27674               24h  i was happy when my son got 90% marks in his e...\n",
       "2  27675               24h     i went to the gym this morning and did yoga . \n",
       "3  27676               24h  we had a serious talk with some friends of our...\n",
       "4  27677               24h  i went with grandchildren to butterfly display..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_hm2'] = df.cleaned_hm.apply(remove_stopwords)\n",
    "df.drop(['cleaned_hm'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(df.cleaned_hm2)\n",
    "VOCAB_SIZE = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60321"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_set = t.texts_to_sequences(df.cleaned_hm2)\n",
    "len(encoded_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>cleaned_hm2</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27673</td>\n",
       "      <td>24h</td>\n",
       "      <td>i went on a successful date with someone i fel...</td>\n",
       "      <td>[1, 21, 16, 3, 762, 314, 11, 282, 1, 90, 12593...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27674</td>\n",
       "      <td>24h</td>\n",
       "      <td>i was happy when my son got 90% marks in his e...</td>\n",
       "      <td>[1, 7, 12, 22, 2, 62, 17, 2277, 1423, 10, 80, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27675</td>\n",
       "      <td>24h</td>\n",
       "      <td>i went to the gym this morning and did yoga .</td>\n",
       "      <td>[1, 21, 4, 6, 393, 37, 91, 5, 101, 929]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27676</td>\n",
       "      <td>24h</td>\n",
       "      <td>we had a serious talk with some friends of our...</td>\n",
       "      <td>[25, 19, 3, 1615, 312, 11, 46, 48, 13, 4418, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27677</td>\n",
       "      <td>24h</td>\n",
       "      <td>i went with grandchildren to butterfly display...</td>\n",
       "      <td>[1, 21, 11, 1903, 4, 3801, 4198, 20, 12595, 12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid reflection_period                                        cleaned_hm2  \\\n",
       "0  27673               24h  i went on a successful date with someone i fel...   \n",
       "1  27674               24h  i was happy when my son got 90% marks in his e...   \n",
       "2  27675               24h     i went to the gym this morning and did yoga .    \n",
       "3  27676               24h  we had a serious talk with some friends of our...   \n",
       "4  27677               24h  i went with grandchildren to butterfly display...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [1, 21, 16, 3, 762, 314, 11, 282, 1, 90, 12593...  \n",
       "1  [1, 7, 12, 22, 2, 62, 17, 2277, 1423, 10, 80, ...  \n",
       "2            [1, 21, 4, 6, 393, 37, 91, 5, 101, 929]  \n",
       "3  [25, 19, 3, 1615, 312, 11, 46, 48, 13, 4418, 1...  \n",
       "4  [1, 21, 11, 1903, 4, 3801, 4198, 20, 12595, 12...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'] = encoded_train_set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27673</td>\n",
       "      <td>24h</td>\n",
       "      <td>[1, 21, 16, 3, 762, 314, 11, 282, 1, 90, 12593...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27674</td>\n",
       "      <td>24h</td>\n",
       "      <td>[1, 7, 12, 22, 2, 62, 17, 2277, 1423, 10, 80, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27675</td>\n",
       "      <td>24h</td>\n",
       "      <td>[1, 21, 4, 6, 393, 37, 91, 5, 101, 929]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27676</td>\n",
       "      <td>24h</td>\n",
       "      <td>[25, 19, 3, 1615, 312, 11, 46, 48, 13, 4418, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27677</td>\n",
       "      <td>24h</td>\n",
       "      <td>[1, 21, 11, 1903, 4, 3801, 4198, 20, 12595, 12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid reflection_period                                             tokens\n",
       "0  27673               24h  [1, 21, 16, 3, 762, 314, 11, 282, 1, 90, 12593...\n",
       "1  27674               24h  [1, 7, 12, 22, 2, 62, 17, 2277, 1423, 10, 80, ...\n",
       "2  27675               24h            [1, 21, 4, 6, 393, 37, 91, 5, 101, 929]\n",
       "3  27676               24h  [25, 19, 3, 1615, 312, 11, 46, 48, 13, 4418, 1...\n",
       "4  27677               24h  [1, 21, 11, 1903, 4, 3801, 4198, 20, 12595, 12..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['cleaned_hm2'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27673</td>\n",
       "      <td>24h</td>\n",
       "      <td>[1, 21, 16, 3, 762, 314, 11, 282, 1, 90, 12593...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27674</td>\n",
       "      <td>24h</td>\n",
       "      <td>[1, 7, 12, 22, 2, 62, 17, 2277, 1423, 10, 80, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27675</td>\n",
       "      <td>24h</td>\n",
       "      <td>[1, 21, 4, 6, 393, 37, 91, 5, 101, 929, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27676</td>\n",
       "      <td>24h</td>\n",
       "      <td>[25, 19, 3, 1615, 312, 11, 46, 48, 13, 4418, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27677</td>\n",
       "      <td>24h</td>\n",
       "      <td>[1, 21, 11, 1903, 4, 3801, 4198, 20, 12595, 12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid reflection_period                                             tokens\n",
       "0  27673               24h  [1, 21, 16, 3, 762, 314, 11, 282, 1, 90, 12593...\n",
       "1  27674               24h  [1, 7, 12, 22, 2, 62, 17, 2277, 1423, 10, 80, ...\n",
       "2  27675               24h  [1, 21, 4, 6, 393, 37, 91, 5, 101, 929, 0, 0, ...\n",
       "3  27676               24h  [25, 19, 3, 1615, 312, 11, 46, 48, 13, 4418, 1...\n",
       "4  27677               24h  [1, 21, 11, 1903, 4, 3801, 4198, 20, 12595, 12..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train_docs = pad_sequences(encoded_train_set, maxlen=SEQ_LEN, padding='post')\n",
    "train_paddocs = []\n",
    "for doc in padded_train_docs:\n",
    "    train_paddocs.append(list(doc))\n",
    "df['tokens2'] = train_paddocs\n",
    "\n",
    "lengths = []\n",
    "for doc in train_paddocs:\n",
    "    lengths.append(len(doc))\n",
    "    \n",
    "print(np.mean(lengths))\n",
    "df.drop(['tokens'], axis=1, inplace=True)\n",
    "df.rename(index=str, columns={'tokens2': 'tokens'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20810\n",
      "Did not find a\n",
      "Did not find to\n",
      "Did not find and\n",
      "Did not find of\n",
      "Did not find \r",
      "\r\n",
      "Did not find 10\n",
      "Did not find mturk\n",
      "Did not find 24\n",
      "Did not find 20\n",
      "Did not find 100\n",
      "Did not find 30\n",
      "Did not find 15\n",
      "Did not find 50\n",
      "Did not find 12\n",
      "Did not find 00\n",
      "Did not find 2017\n",
      "Did not find 25\n",
      "Did not find 40\n",
      "Did not find 11\n",
      "Did not find 500\n",
      "Did not find 13\n",
      "Did not find 000\n",
      "Did not find 200\n",
      "Did not find i'll\n",
      "Did not find donat\n",
      "Did not find 14\n",
      "Did not find 300\n",
      "Did not find 60\n",
      "Did not find travelling\n",
      "Did not find judgements\n",
      "Did not find 250\n",
      "Did not find 18\n",
      "Did not find 80\n",
      "Did not find 10th\n",
      "Did not find 1000\n",
      "Did not find 45\n",
      "Did not find 16\n",
      "Did not find 70\n",
      "Did not find 17\n",
      "Did not find 90\n",
      "Did not find morty\n",
      "Did not find ooty\n",
      "Did not find 2000\n",
      "Did not find ps4\n",
      "Did not find 75\n",
      "Did not find fianca\n",
      "Did not find 30th\n",
      "Did not find eudaimonia\n",
      "Did not find jigarthanda\n",
      "Did not find a3i\n",
      "Did not find iave\n",
      "Did not find 2016\n",
      "Did not find 150\n",
      "Did not find 99\n",
      "Did not find thatas\n",
      "Did not find learnt\n",
      "Did not find eudaimonic\n",
      "Did not find wayanad\n",
      "Did not find '\n",
      "Did not find 35\n",
      "Did not find 22\n",
      "Did not find seligman\n",
      "Did not find 21\n",
      "Did not find ruut\n",
      "Did not find veenhoven\n",
      "Did not find 400\n",
      "Did not find honour\n",
      "Did not find 20th\n",
      "Did not find instagram\n",
      "Did not find 600\n",
      "Did not find gonzaga\n",
      "Did not find amartya\n",
      "Did not find diener\n",
      "Did not find 19\n",
      "Did not find 32\n",
      "Did not find 5000\n",
      "Did not find munnar\n",
      "Did not find mnc\n",
      "Did not find 120\n",
      "Did not find 700\n",
      "Did not find 25th\n",
      "Did not find 2005\n",
      "Did not find couldnat\n",
      "Did not find 34\n",
      "Did not find tioman\n",
      "Did not find 350\n",
      "Did not find life\r",
      "\r\n",
      "Did not find bahubali\n",
      "Did not find 800\n",
      "Did not find 3000\n",
      "Did not find turking\n",
      "Did not find 96\n",
      "Did not find 28th\n",
      "Did not find 1998\n",
      "Did not find organisation\n",
      "Did not find facetimed\n",
      "Did not find 21st\n",
      "Did not find others'\n",
      "Did not find giftz\n",
      "Did not find canat\n",
      "Did not find 33\n",
      "Did not find time\r",
      "\r\n",
      "Did not find tempel\n",
      "Did not find day\r",
      "\r\n",
      "Did not find 9and\n",
      "Did not find 10class\n",
      "Did not find was\r",
      "\r\n",
      "Did not find ielts\n",
      "Did not find 98\n",
      "Did not find 97\n",
      "Did not find selfie\n",
      "Did not find tirupati\n",
      "Did not find realise\n",
      "Did not find spotify\n",
      "Did not find 31\n",
      "Did not find 23\n",
      "Did not find is\r",
      "\r\n",
      "Did not find 14th\n",
      "Did not find divar\n",
      "Did not find thekkady\n",
      "Did not find vape\n",
      "Did not find publix\n",
      "Did not find fitbit\n",
      "Did not find 23rd\n",
      "Did not find 10k\n",
      "Did not find snapchat\n",
      "Did not find 65\n",
      "Did not find 18th\n",
      "Did not find 93\n",
      "Did not find whatsapp\n",
      "Did not find panera\n",
      "Did not find definitely\r",
      "\r\n",
      "Did not find youall\n",
      "Did not find 95\n",
      "Did not find tessa\n",
      "Did not find 24th\n",
      "Did not find wasnat\n",
      "Did not find 55\n",
      "Did not find 15th\n",
      "Did not find 28\n",
      "Did not find doughnuts\n",
      "Did not find 360\n",
      "Did not find 30pm\n",
      "Did not find minecraft\n",
      "Did not find 2012\n",
      "Did not find 2014\n",
      "Did not find becz\n",
      "Did not find 16th\n",
      "Did not find movent\n",
      "Did not find concened\n",
      "Did not find nerous\n",
      "Did not find studdied\n",
      "Did not find anju\n",
      "Did not find othersa\n",
      "Did not find pharrell\n",
      "Did not find 98th\n",
      "Did not find 17th\n",
      "Did not find youtuber\n",
      "Did not find 13th\n",
      "Did not find doughnut\n",
      "Did not find 80th\n",
      "Did not find madurai\n",
      "Did not find sedona\n",
      "Did not find when\r",
      "\r\n",
      "Did not find felt\r",
      "\r\n",
      "Did not find 94\n",
      "Did not find kannda\n",
      "Did not find after\r",
      "\r\n",
      "Did not find bannon\n",
      "Did not find kendrick\n",
      "Did not find sitted\n",
      "Did not find shakeel\n",
      "Did not find thereas\n",
      "Did not find aiam\n",
      "Did not find 'i\n",
      "Did not find 1500\n",
      "Did not find 1200\n",
      "Did not find 26\n",
      "Did not find 91\n",
      "Did not find kodaikanal\n",
      "Did not find bitcoins\n",
      "Did not find 85\n",
      "Did not find 60th\n",
      "Did not find 2013\n",
      "Did not find disneyworld\n",
      "Did not find mersing\n",
      "Did not find panjim\n",
      "Did not find kroger\n",
      "Did not find 40th\n",
      "Did not find 1776\n",
      "Did not find 900\n",
      "Did not find treacher\n",
      "Did not find dehradun\n",
      "Did not find forgt\n",
      "Did not find gvn\n",
      "Did not find a\r",
      "\r\n",
      "Did not find 90th\n",
      "Did not find aruba\n",
      "Did not find mysore\n",
      "Did not find friends'\n",
      "Did not find 401k\n",
      "Did not find 26th\n",
      "Did not find chappelle\n",
      "Did not find 43\n",
      "Did not find 46\n",
      "Did not find job\r",
      "\r\n",
      "Did not find 80s\n",
      "Did not find 72\n",
      "Did not find 37\n",
      "Did not find 89\n",
      "Did not find iaam\n",
      "Did not find meenakshi\n",
      "Did not find feling\n",
      "Did not find care''\n",
      "Did not find licence\n",
      "Did not find 220\n",
      "Did not find ihop\n",
      "Did not find 36\n",
      "Did not find 'happy\n",
      "Did not find 92\n",
      "Did not find dreamt\n",
      "Did not find boss'\n",
      "Did not find 38\n",
      "Did not find 50th\n",
      "Did not find 'the\n",
      "Did not find amutha\n",
      "Did not find sephora\n",
      "Did not find lyft\n",
      "Did not find febuary\n",
      "Did not find 29th\n",
      "Did not find 'yes'\n",
      "Did not find 12th\n",
      "Did not find youa\n",
      "Did not find amumma\n",
      "Did not find awhy\n",
      "Did not find rotia\n",
      "Did not find balaji\n",
      "Did not find having\r",
      "\r\n",
      "Did not find arenat\n",
      "Did not find centre\n",
      "Did not find 24hrs\n",
      "Did not find 2008\n",
      "Did not find 88\n",
      "Did not find 70th\n",
      "Did not find party\r",
      "\r\n",
      "Did not find dunkin'\n",
      "Did not find applebee\n",
      "Did not find waltonsa\n",
      "Did not find fanduel\n",
      "Did not find nier\n",
      "Did not find goldbergs\n",
      "Did not find 00am\n",
      "Did not find manali\n",
      "Did not find bihu\n",
      "Did not find 27\n",
      "Did not find cadbury\n",
      "Did not find 22nd\n",
      "Did not find 180\n",
      "Did not find bitcoin\n",
      "Did not find i'\n",
      "Did not find 03\n",
      "Did not find 39\n",
      "Did not find quora\n",
      "Did not find 1999\n",
      "Did not find ''have\n",
      "Did not find ineffability\n",
      "Did not find jeti\n",
      "Did not find whisky\n",
      "Did not find 70s\n",
      "Did not find fransisco\n",
      "Did not find 'in\n",
      "Did not find ptsd\n",
      "Did not find birthday'\n",
      "Did not find arace\n",
      "Did not find othera\n",
      "Did not find 81\n",
      "Did not find wonderla\n",
      "Did not find 10000\n",
      "Did not find soumya\n",
      "Did not find youtubers\n",
      "Did not find inlove\n",
      "Did not find 104\n",
      "Did not find marathoned\n",
      "Did not find hajer\n",
      "Did not find tball\n",
      "Did not find ahow\n",
      "Did not find waltons\n",
      "Did not find 54\n",
      "Did not find a1c\n",
      "Did not find bava\n",
      "Did not find 29\n",
      "Did not find aquinas\n",
      "Did not find 05\n",
      "Did not find comey\n",
      "Did not find wonat\n",
      "Did not find perigourdine\n",
      "Did not find tyre\n",
      "Did not find ugadi\n",
      "Did not find 225\n",
      "Did not find 1960s\n",
      "Did not find finlay\n",
      "Did not find selfies\n",
      "Did not find zootopia\n",
      "Did not find 42\n",
      "Did not find jamba\n",
      "Did not find 37th\n",
      "Did not find 450\n",
      "Did not find 115\n",
      "Did not find 195\n",
      "Did not find flynn\n",
      "Did not find sivan\n",
      "Did not find spacex\n",
      "Did not find letas\n",
      "Did not find 2006\n",
      "Did not find cheque\n",
      "Did not find gaiaonline\n",
      "Did not find 00pm\n",
      "Did not find cancun\n",
      "Did not find silkie\n",
      "Did not find ''\n",
      "Did not find janeiro\n",
      "Did not find favour\n",
      "Did not find buyed\n",
      "Did not find andriod\n",
      "Did not find monisha\n",
      "Did not find coimbatore\n",
      "Did not find r15\n",
      "Did not find heas\n",
      "Did not find afraiding\n",
      "Did not find lakshi\n",
      "Did not find samoa\n",
      "Did not find stardew\n",
      "Did not find 11months\n",
      "Did not find funko\n",
      "Did not find gaiman\n",
      "Did not find edm\n",
      "Did not find kodi\n",
      "Did not find brotheras\n",
      "Did not find postmates\n",
      "Did not find arby\n",
      "Did not find futurama\n",
      "Did not find grey\n",
      "Did not find h1z1\n",
      "Did not find 48\n",
      "Did not find realised\n",
      "Did not find 60k\n",
      "Did not find vons\n",
      "Did not find 'how\n",
      "Did not find dryden\n",
      "Did not find 10am\n",
      "Did not find shinsuke\n",
      "Did not find kreme\n",
      "Did not find 68\n",
      "Did not find kailua\n",
      "Did not find 365\n",
      "Did not find 1993\n",
      "Did not find dbz\n",
      "Did not find pinterest\n",
      "Did not find 'n\n",
      "Did not find cryptocurrency\n",
      "Did not find comicon\n",
      "Did not find students'\n",
      "Did not find 77\n",
      "Did not find 64\n",
      "Did not find blt\n",
      "Did not find 4000\n",
      "Did not find cdl\n",
      "Did not find snorlax\n",
      "Did not find airbnb\n",
      "Did not find 'free\n",
      "Did not find 10lbs\n",
      "Did not find sheldon\n",
      "Did not find nowa\n",
      "Did not find 6000\n",
      "Did not find tybee\n",
      "Did not find soylent\n",
      "Did not find jehovah\n",
      "Did not find eeg\n",
      "Did not find chating\n",
      "Did not find happy\r",
      "\r\n",
      "Did not find friend\r",
      "\r\n",
      "Did not find 160\n",
      "Did not find valentineas\n",
      "Did not find tajmahal\n",
      "Did not find 86\n",
      "Did not find 1300\n",
      "Did not find dogsit\n",
      "Did not find tefl\n",
      "Did not find mccartney\n",
      "Did not find achoras\n",
      "Did not find ariyaman\n",
      "Did not find ramanathapuram\n",
      "Did not find expectations\r",
      "\r\n",
      "Did not find shocka\n",
      "Did not find galway\n",
      "Did not find belize\n",
      "Did not find 11th\n",
      "Did not find 275\n",
      "Did not find kitkat\n",
      "Did not find programr\n",
      "Did not find nisaar\n",
      "Did not find teena\n",
      "Did not find fenway\n",
      "Did not find ieps\n",
      "Did not find 130\n",
      "Did not find wead\n",
      "Did not find basav\n",
      "Did not find 27th\n",
      "Did not find gorillaz\n",
      "Did not find daston\n",
      "Did not find periyar\n",
      "Did not find ktdc\n",
      "Did not find 108\n",
      "Did not find tajas\n",
      "Did not find yamuna\n",
      "Did not find amritsar\n",
      "Did not find doesnat\n",
      "Did not find appalachian\n",
      "Did not find quadcopter\n",
      "Did not find enfield\n",
      "Did not find 1600\n",
      "Did not find iall\n",
      "Did not find pudding\r",
      "\r\n",
      "Did not find applebees\n",
      "Did not find 125\n",
      "Did not find success\r",
      "\r\n",
      "Did not find bourdon\n",
      "Did not find degeneres\n",
      "Did not find dnd\n",
      "Did not find tearja\n",
      "Did not find them\r",
      "\r\n",
      "Did not find portillo\n",
      "Did not find 20s\n",
      "Did not find kottayam\n",
      "Did not find lyling\n",
      "Did not find 170\n",
      "Did not find 31st\n",
      "Did not find bengaluru\n",
      "Did not find chapelle\n",
      "Did not find graceland\n",
      "Did not find ocd\n",
      "Did not find family\r",
      "\r\n",
      "Did not find 64gb\n",
      "Did not find orthopaedic\n",
      "Did not find 4x4\n",
      "Did not find woobie\n",
      "Did not find markiplier\n",
      "Did not find khonoma\n",
      "Did not find nagaland\n",
      "Did not find pervia\n",
      "Did not find augustine\n",
      "Did not find shiridi\n",
      "Did not find 1852\n",
      "Did not find capao\n",
      "Did not find canoa\n",
      "Did not find judgement\n",
      "Did not find dhubai\n",
      "Did not find 18k\n",
      "Did not find papper\n",
      "Did not find souplantation\n",
      "Did not find booka\n",
      "Did not find rainning\n",
      "Did not find 25000\n",
      "Did not find 15am\n",
      "Did not find 2018\n",
      "Did not find room'\n",
      "Did not find jongg\n",
      "Did not find kratom\n",
      "Did not find rickman\n",
      "Did not find liffe\n",
      "Did not find tirupti\n",
      "Did not find draftkings\n",
      "Did not find konosuba\n",
      "Did not find 24hours\n",
      "Did not find ajith\n",
      "Did not find ferndale\n",
      "Did not find sazelia\n",
      "Did not find diamandis\n",
      "Did not find 30am\n",
      "Did not find edna\n",
      "Did not find asmr\n",
      "Did not find endeavour\n",
      "Did not find mettupalayam\n",
      "Did not find 286\n",
      "Did not find 1819\n",
      "Did not find combater\n",
      "Did not find englishmen\n",
      "Did not find 638\n",
      "Did not find mariamman\n",
      "Did not find 50000\n",
      "Did not find mechwarrior\n",
      "Did not find 140\n",
      "Did not find dayas\n",
      "Did not find 58\n",
      "Did not find bronson\n",
      "Did not find baskin\n",
      "Did not find kyrgios\n",
      "Did not find 119\n",
      "Did not find maddow\n",
      "Did not find 87\n",
      "Did not find coldstone\n",
      "Did not find knowledgeas\n",
      "Did not find asuccess\n",
      "Did not find secretsa\n",
      "Did not find vips\n",
      "Did not find wwii\n",
      "Did not find whatching\n",
      "Did not find bojack\n",
      "Did not find wilco\n",
      "Did not find gudaloor\n",
      "Did not find yahtzee\n",
      "Did not find comady\n",
      "Did not find byk\n",
      "Did not find benihana\n",
      "Did not find johor\n",
      "Did not find bharu\n",
      "Did not find tinggi\n",
      "Did not find tiomal\n",
      "Did not find grimm\n",
      "Did not find heneiken\n",
      "Did not find chappathi\n",
      "Did not find aguada\n",
      "Did not find 240\n",
      "Did not find fools'\n",
      "Did not find hansika\n",
      "Did not find kanyakumari\n",
      "Did not find goodmorning\n",
      "Did not find 180s\n",
      "Did not find jenifer\n",
      "Did not find 2003\n",
      "Did not find axe\n",
      "Did not find grandpaas\n",
      "Did not find gnomoria\n",
      "Did not find 701\n",
      "Did not find olfus\n",
      "Did not find societyas\n",
      "Did not find 83\n",
      "Did not find geico\n",
      "Did not find guys'\n",
      "Did not find temble\n",
      "Did not find whitecastle\n",
      "Did not find 'to\n",
      "Did not find astley\n",
      "Did not find svu\n",
      "Did not find eggo\n",
      "Did not find konmari\n",
      "Did not find yosemite\n",
      "Did not find turker\n",
      "Did not find isnat\n",
      "Did not find nature\r",
      "\r\n",
      "Did not find titanfall\n",
      "Did not find zaxby\n",
      "Did not find keatsa\n",
      "Did not find keats\n",
      "Did not find 'their\n",
      "Did not find bluerose\n",
      "Did not find 2ds\n",
      "Did not find bikeriding\n",
      "Did not find saviour\n",
      "Did not find nakamura\n",
      "Did not find 11pm\n",
      "Did not find varry\n",
      "Did not find enjing\n",
      "Did not find szechuan\n",
      "Did not find fawlty\n",
      "Did not find ingmar\n",
      "Did not find rameswaram\n",
      "Did not find ganesan\n",
      "Did not find banglore\n",
      "Did not find bharathanatyam\n",
      "Did not find abhai\n",
      "Did not find mazak\n",
      "Did not find cgl\n",
      "Did not find 2400\n",
      "Did not find 15a30\n",
      "Did not find 63\n",
      "Did not find mylie\n",
      "Did not find mturker\n",
      "Did not find 30min\n",
      "Did not find hasnt\n",
      "Did not find 'better\n",
      "Did not find saul'\n",
      "Did not find kathe\n",
      "Did not find snapchats\n",
      "Did not find todds\n",
      "Did not find flow'\n",
      "Did not find 23andme\n",
      "Did not find dortmund\n",
      "Did not find meenatchi\n",
      "Did not find 135\n",
      "Did not find joyfull\n",
      "Did not find 'become\n",
      "Did not find fiance'\n",
      "Did not find okcupid\n",
      "Did not find waggaki\n",
      "Did not find smartwatch\n",
      "Did not find nsc\n",
      "Did not find icee\n",
      "Did not find 102\n",
      "Did not find 750\n",
      "Did not find cpap\n",
      "Did not find 2010\n",
      "Did not find 1week\n",
      "Did not find mooc\n",
      "Did not find enquire\n",
      "Did not find bycycle\n",
      "Did not find flavoured\n",
      "Did not find duolingo\n",
      "Did not find 42nd\n",
      "Did not find woodbee\n",
      "Did not find rubik\n",
      "Did not find 56\n",
      "Did not find coffiest\n",
      "Did not find gracie\n",
      "Did not find popeye\n",
      "Did not find kovilpatti\n",
      "Did not find layla\n",
      "Did not find reading'\n",
      "Did not find wuthering\n",
      "Did not find them'\n",
      "Did not find mst3k\n",
      "Did not find 101\n",
      "Did not find 47\n",
      "Did not find nieces'\n",
      "Did not find 1400\n",
      "Did not find 115lbs\n",
      "Did not find ootty\n",
      "Did not find timesthat\n",
      "Did not find sherri\n",
      "Did not find vudu\n",
      "Did not find 16000\n",
      "Did not find 775\n",
      "Did not find you'\n",
      "Did not find 35th\n",
      "Did not find college\r",
      "\r\n",
      "Did not find varkala\n",
      "Did not find psg\n",
      "Did not find porygon\n",
      "Did not find nathaniel\n",
      "Did not find rateliff\n",
      "Did not find pictionary\n",
      "Did not find 182\n",
      "Did not find uil\n",
      "Did not find 20mins\n",
      "Did not find ringling\n",
      "Did not find ellensburg\n",
      "Did not find 04\n",
      "Did not find fmla\n",
      "Did not find 2015\n",
      "Did not find boyfriend\r",
      "\r\n",
      "Did not find 127\n",
      "Did not find wein\n",
      "Did not find gitchi\n",
      "Did not find friends\r",
      "\r\n",
      "Did not find pembroke\n",
      "Did not find bouth\n",
      "Did not find aadhaar\n",
      "Did not find reclast\n",
      "Did not find 420\n",
      "Did not find t20\n",
      "Did not find 32nd\n",
      "Did not find isnt\n",
      "Did not find 85th\n",
      "Did not find udacity\n",
      "Did not find holika\n",
      "Did not find 550\n",
      "Did not find 'well\n",
      "Did not find th3\n",
      "Did not find valentina\n",
      "Did not find 2011\n",
      "Did not find kors\n",
      "Did not find kovalam\n",
      "Did not find nicco\n",
      "Did not find hortons\n",
      "Did not find 'dada'\n",
      "Did not find febraury\n",
      "Did not find gretchen\n",
      "Did not find gogh\n",
      "Did not find wanee\n",
      "Did not find griffith\n",
      "Did not find not'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not find keats'\n",
      "Did not find aashwin\n",
      "Did not find chainsmoker\n",
      "Did not find youad\n",
      "Did not find aoh\n",
      "Did not find pulwama\n",
      "Did not find 3mths\n",
      "Did not find udemy\n",
      "Did not find 'reverse\n",
      "Did not find danville\n",
      "Did not find rachael\n",
      "Did not find abeading\n",
      "Did not find it\r",
      "\r\n",
      "Did not find 66\n",
      "Did not find obgyn\n",
      "Did not find isbell\n",
      "Did not find perumal\n",
      "Did not find taskrabbit\n",
      "Did not find bhagubali\n",
      "Did not find 190\n",
      "Did not find festival'\n",
      "Did not find 110\n",
      "Did not find feeled\n",
      "Did not find taipei\n",
      "Did not find labtop\n",
      "Did not find chromecast\n",
      "Did not find 30p\n",
      "Did not find tirumala\n",
      "Did not find comedians'\n",
      "Did not find show'\n",
      "Did not find enkement\n",
      "Did not find gova\n",
      "Did not find employeesa\n",
      "Did not find sons'\n",
      "Did not find plr\n",
      "Did not find favourable\n",
      "Did not find favrote\n",
      "Did not find alton\n",
      "Did not find ziva\n",
      "Did not find surajkund\n",
      "Did not find haagen\n",
      "Did not find bhk\n",
      "Did not find tumour\n",
      "Did not find beaufort\n",
      "Did not find attravtive\n",
      "Did not find shakeology\n",
      "Did not find enjojment\n",
      "Did not find hesi\n",
      "Did not find kumily\n",
      "Did not find mullapperiyar\n",
      "Did not find hardees\n",
      "Did not find kerb\n",
      "Did not find conservatory\r",
      "\r\n",
      "Did not find bcaas\n",
      "Did not find aminos\n",
      "Did not find woker\n",
      "Did not find carley\n",
      "Did not find jepsen\n",
      "Did not find chicken\r",
      "\r\n",
      "Did not find suryanamaskars\n",
      "Did not find trainas\n",
      "Did not find mahalas\n",
      "Did not find aagra\n",
      "Did not find aeast\n",
      "Did not find indiaas\n",
      "Did not find sheas\n",
      "Did not find i\r",
      "\r\n",
      "Did not find child\r",
      "\r\n",
      "Did not find quadcopters\n",
      "Did not find aadhar\n",
      "Did not find cubs'\n",
      "Did not find 85kg\n",
      "Did not find samsonite\n",
      "Did not find 'bedtime\n",
      "Did not find surprise'\n",
      "Did not find elsinore\n",
      "Did not find gim\n",
      "Did not find zbrush\n",
      "Did not find djed\n",
      "Did not find padio\n",
      "Did not find 315\n",
      "Did not find playing\r",
      "\r\n",
      "Did not find sheryl\n",
      "Did not find lindt\n",
      "Did not find meeting\r",
      "\r\n",
      "Did not find keandra\n",
      "Did not find surveys\r",
      "\r\n",
      "Did not find trick\r",
      "\r\n",
      "Did not find rmx\n",
      "Did not find japaneses\n",
      "Did not find pupper\n",
      "Did not find creditkarma\n",
      "Did not find that'd\n",
      "Did not find hersheys\n",
      "Did not find phone\r",
      "\r\n",
      "Did not find cht\n",
      "Did not find chickfila\n",
      "Did not find partner\r",
      "\r\n",
      "Did not find birthday\r",
      "\r\n",
      "Did not find 200k\n",
      "Did not find yessica\n",
      "Did not find mindy\n",
      "Did not find ptkgrecpi1i\n",
      "Did not find ice\r",
      "\r\n",
      "Did not find series\r",
      "\r\n",
      "Did not find hersh\n",
      "Did not find somenthing\n",
      "Did not find ceasars\n",
      "Did not find roleplayed\n",
      "Did not find engagement\r",
      "\r\n",
      "Did not find temple\r",
      "\r\n",
      "Did not find marriage\r",
      "\r\n",
      "Did not find enjoyful\n",
      "Did not find procert\n",
      "Did not find norvin\n",
      "Did not find pgy2\n",
      "Did not find slothrust\n",
      "Did not find patiantly\n",
      "Did not find ahey\n",
      "Did not find hereas\n",
      "Did not find 'munar\n",
      "Did not find talesa\n",
      "Did not find kanan\n",
      "Did not find devan\n",
      "Did not find msa\n",
      "Did not find fauno\n",
      "Did not find windermere\n",
      "Did not find antony\n",
      "Did not find iching\n",
      "Did not find animels\n",
      "Did not find beutifull\n",
      "Did not find 'on\n",
      "Did not find job'\n",
      "Did not find cude\n",
      "Did not find v60\n",
      "Did not find kulu\n",
      "Did not find 186\n",
      "Did not find medel\n",
      "Did not find verymuch\n",
      "Did not find manytimes\n",
      "Did not find apfc\n",
      "Did not find 147\n",
      "Did not find patrickas\n",
      "Did not find nugs\n",
      "Did not find pgcert\n",
      "Did not find retreiver\n",
      "Did not find laughlin\n",
      "Did not find robinhood\n",
      "Did not find ffa\n",
      "Did not find salry\n",
      "Did not find gramado\n",
      "Did not find founders'\n",
      "Did not find laurenta\n",
      "Did not find grandsona\n",
      "Did not find marthon\n",
      "Did not find while\r",
      "\r\n",
      "Did not find espcas9\n",
      "Did not find cas9\n",
      "Did not find bataan\n",
      "Did not find gryo\r",
      "\r\n",
      "Did not find you\r",
      "\r\n",
      "Did not find pingami\n",
      "Did not find placea\n",
      "Did not find rainers\n",
      "Did not find fanas\n",
      "Did not find damodara\n",
      "Did not find touche'\n",
      "Did not find horton\n",
      "Did not find whwn\n",
      "Did not find sprung'\n",
      "Did not find bibbi\n",
      "Did not find 2a3\n",
      "Did not find lotery\n",
      "Did not find 84th\n",
      "Did not find insatllation\n",
      "Did not find 2200\n",
      "Did not find petey\n",
      "Did not find 153\n",
      "Did not find bakasana\n",
      "Did not find chuckie\n",
      "Did not find ggod\n",
      "Did not find loyal3\n",
      "Did not find clup\n",
      "Did not find frenchies\n",
      "Did not find bffs\n",
      "Did not find noddle\n",
      "Did not find educatiion\n",
      "Did not find wondercon\n",
      "Did not find psat\n",
      "Did not find shortfilm\n",
      "Did not find ryzen\n",
      "Did not find babysiting\n",
      "Did not find powerwalked\n",
      "Did not find 20as\n",
      "Did not find 30as\n",
      "Did not find selfieeeeee\n",
      "Did not find 'halfway\n",
      "Did not find fifty'\n",
      "Did not find apartmentas\n",
      "Did not find aplease\n",
      "Did not find allaha\n",
      "Did not find beehouse\n",
      "Did not find austalia\n",
      "Did not find patricks\n",
      "Did not find 'elephant\n",
      "Did not find doppelga\n",
      "Did not find crowdlab\n",
      "Did not find 35mph\n",
      "Did not find barkfast\n",
      "Did not find dennar\n",
      "Did not find traviling\n",
      "Did not find orkavt\n",
      "Did not find zafu\n",
      "Did not find guayama\n",
      "Did not find increament\n",
      "Did not find withinaaafrom\n",
      "Did not find whyaaanot\n",
      "Did not find asideaaahealthy\n",
      "Did not find headachesaaawhich\n",
      "Did not find placeboaaareplace\n",
      "Did not find 1500m\n",
      "Did not find materialised\n",
      "Did not find 'cheeseburger\n",
      "Did not find quesadillas'\n",
      "Did not find favarat\n",
      "Did not find lince\n",
      "Did not find ladery\n",
      "Did not find prise\n",
      "Did not find worriedy\n",
      "Did not find casesaaamaking\n",
      "Did not find worldas\n",
      "Did not find 50mb\n",
      "Did not find ppppppublic\n",
      "Did not find millay\n",
      "Did not find genderfluid\n",
      "Did not find obstipation\n",
      "Did not find cashman\n",
      "Did not find nights'\n",
      "Did not find magnadoodle\n",
      "Did not find thoreau\n",
      "Did not find 'nothing\n",
      "Did not find troubles'\n",
      "Did not find homedown\n",
      "Did not find bs4\n",
      "Did not find popeyes\n",
      "Did not find alayah\n",
      "Did not find shadowverse\n",
      "Did not find aldi\n",
      "Did not find porota\n",
      "Did not find overajoyed\n",
      "Did not find aqueen\n",
      "Did not find stationsa\n",
      "Did not find samas\n",
      "Did not find sarisa\n",
      "Did not find aps3000\n",
      "Did not find personally\r",
      "\r\n",
      "Did not find 01\n",
      "Did not find keerthana\n",
      "Did not find meenu\n",
      "Did not find tomlinson\n",
      "Did not find ufhd\n",
      "Did not find yoohoo\n",
      "Did not find vaccumed\n",
      "Did not find jrotc\n",
      "Did not find wendys\n",
      "Did not find deposite\n",
      "Did not find waight\n",
      "Did not find adegt\n",
      "Did not find stresful\n",
      "Did not find mturking\n",
      "Did not find 151\n",
      "Did not find crunchyroll\n",
      "Did not find tolstoy\n",
      "Did not find 803\n",
      "Did not find cutte\n",
      "Did not find alchia\n",
      "Did not find prine\n",
      "Did not find 1987\n",
      "Did not find 1980s\n",
      "Did not find binghamton\n",
      "Did not find hattiesburg\n",
      "Did not find normandy\n",
      "Did not find scranton\n",
      "Did not find its'\n",
      "Did not find ad'\n",
      "Did not find 'true\n",
      "Did not find detective'\n",
      "Did not find cybrary\n",
      "Did not find disucssing\n",
      "Did not find mandarines\n",
      "Did not find jobe\n",
      "Did not find tohoshinki\n",
      "Did not find colion\n",
      "Did not find 514\n",
      "Did not find ambitiousareal\n",
      "Did not find videosaand\n",
      "Did not find crisisawhen\n",
      "Did not find better\r",
      "\r\n",
      "Did not find 24h\n",
      "Did not find yonex\n",
      "Did not find lanudry\n",
      "Did not find nths\n",
      "Did not find zverev\n",
      "Did not find dibels\n",
      "Did not find playmateeeeee\n",
      "Did not find jhon\n",
      "Did not find gaffigan\n",
      "Did not find asma\n",
      "Did not find tastey\n",
      "Did not find esport\n",
      "Did not find 'dad'\n",
      "Did not find krav\n",
      "Did not find 'cold'\n",
      "Did not find chappel\n",
      "Did not find meditaion\n",
      "Did not find wendt\n",
      "Did not find wingstop\n",
      "Did not find pollar\n",
      "Did not find asteala\n",
      "Did not find 2k8\n",
      "Did not find individualas\n",
      "Did not find rudolph\n",
      "Did not find autry\n",
      "Did not find godspell\n",
      "Did not find daughn\n",
      "Did not find chopin\n",
      "Did not find pisode\n",
      "Did not find permanet\n",
      "Did not find 41\n",
      "Did not find greys\n",
      "Did not find appying\n",
      "Did not find view'\n",
      "Did not find 10th\r",
      "\r\n",
      "Did not find 1950\n",
      "Did not find r15bike\n",
      "Did not find wafla\n",
      "Did not find chekhov\n",
      "Did not find getting'\n",
      "Did not find hasnat\n",
      "Did not find ulties\n",
      "Did not find rescources\n",
      "Did not find scarace\n",
      "Did not find ugadhi\n",
      "Did not find lakkidy\n",
      "Did not find lakkidi\n",
      "Did not find thamarassery\n",
      "Did not find ramya\n",
      "Did not find vadivel\n",
      "Did not find santhanam\n",
      "Did not find humour\n",
      "Did not find colorfull\n",
      "Did not find bleesed\n",
      "Did not find ulip\n",
      "Did not find 2mths\n",
      "Did not find 'biryani'\n",
      "Did not find apologise\n",
      "Did not find receipes\n",
      "Did not find jamiroquai\n",
      "Did not find yahooo\n",
      "Did not find flyingggggg\n",
      "Did not find bodypump\n",
      "Did not find speding\n",
      "Did not find deathstalker\n",
      "Did not find calangute\n",
      "Did not find anjuna\n",
      "Did not find vagator\n",
      "Did not find chapora\n",
      "Did not find assisis\n",
      "Did not find mangeshi\n",
      "Did not find gapanati\n",
      "Did not find mulund\n",
      "Did not find vaibhavs\n",
      "Did not find kohlapur\n",
      "Did not find amboli\n",
      "Did not find framcis\n",
      "Did not find 1594\n",
      "Did not find 1623\n",
      "Did not find ignatius\n",
      "Did not find 1624\n",
      "Did not find 1655\n",
      "Did not find brandizzi\n",
      "Did not find 1955\n",
      "Did not find jesuits\n",
      "Did not find wynonna\n",
      "Did not find earp\n",
      "Did not find thiess\n",
      "Did not find uncw\n",
      "Did not find motwani\n",
      "Did not find remarkablefor\n",
      "Did not find haert\n",
      "Did not find doar\n",
      "Did not find jeery\n",
      "Did not find dangal\n",
      "Did not find bareburger\n",
      "Did not find scopus\n",
      "Did not find slovenia\n",
      "Did not find gambrill\n",
      "Did not find rubix\n",
      "Did not find 'three\n",
      "Did not find birds'\n",
      "Did not find 107\n",
      "Did not find silverstein\n",
      "Did not find asuka\n",
      "Did not find sweeteset\n",
      "Did not find 480\n",
      "Did not find biece\n",
      "Did not find underneathhhhhh\n",
      "Did not find tusken\n",
      "Did not find bantha\n",
      "Did not find zillow\n",
      "Did not find wiiu\n",
      "Did not find saterday\n",
      "Did not find lide\n",
      "Did not find schlotzsky\n",
      "Did not find 327\n",
      "Did not find kenilworth\n",
      "Did not find ceylon\n",
      "Did not find jollof\n",
      "Did not find 10a12\n",
      "Did not find 3a4\n",
      "Did not find edition\r",
      "\r\n",
      "Did not find spoonlickers\n",
      "Did not find 393\n",
      "Did not find microwork\n",
      "Did not find galactica\n",
      "Did not find shyamalan\n",
      "Did not find pokeman\n",
      "Did not find rueben\n",
      "Did not find directtv\n",
      "Did not find mongeau\n",
      "Did not find tarful\n",
      "Did not find palpatine\n",
      "Did not find takei\n",
      "Did not find keegan\n",
      "Did not find tulalip\n",
      "Did not find fivecentsmatter\n",
      "Did not find 'why'\n",
      "Did not find 129\n",
      "Did not find 'inside\n",
      "Did not find out'\n",
      "Did not find condition'\n",
      "Did not find socialise\n",
      "Did not find 25ac\n",
      "Did not find twords\n",
      "Did not find snailzilla\n",
      "Did not find nightout\n",
      "Did not find roode\n",
      "Did not find shinskue\n",
      "Did not find nakumara\n",
      "Did not find gwenpool\n",
      "Did not find anymoe\n",
      "Did not find phelpas\n",
      "Did not find konta\n",
      "Did not find wosniaky\n",
      "Did not find bjorkstrand\n",
      "Did not find gutfeld\n",
      "Did not find callie\n",
      "Did not find 55pm\n",
      "Did not find ncis\n",
      "Did not find 545\n",
      "Did not find 2011a12\n",
      "Did not find 1340\n",
      "Did not find seagrams\n",
      "Did not find horsley\n",
      "Did not find 1965\n",
      "Did not find mizell\n",
      "Did not find perren\n",
      "Did not find footgolf\n",
      "Did not find ''me\n",
      "Did not find time''\n",
      "Did not find udf\n",
      "Did not find microjobs\n",
      "Did not find compex\n",
      "Did not find goddard\n",
      "Did not find sikkim\n",
      "Did not find hollywoody\n",
      "Did not find yotsuba\n",
      "Did not find krakauer\n",
      "Did not find tarheels\n",
      "Did not find bany\n",
      "Did not find nissi\n",
      "Did not find facetimeing\n",
      "Did not find bananagrams\n",
      "Did not find ezekiel\n",
      "Did not find mactch\n",
      "Did not find 84\n",
      "Did not find ukg\n",
      "Did not find wedding\r",
      "\r\n",
      "Did not find pokemons\n",
      "Did not find pokecoins\n",
      "Did not find residentsa\n",
      "Did not find namrata\n",
      "Did not find 52\n",
      "Did not find meghan\n",
      "Did not find lovel\n",
      "Did not find krystal\n",
      "Did not find odysseus\n",
      "Did not find crunchwrap\n",
      "Did not find simcity\n",
      "Did not find naperville\n",
      "Did not find do'\n",
      "Did not find bunyon\n",
      "Did not find unaccomplishment\n",
      "Did not find wrestlmania\n",
      "Did not find vz\n",
      "Did not find orojects\n",
      "Did not find fafsa\n",
      "Did not find berkey\n",
      "Did not find gritz\n",
      "Did not find bacardi\n",
      "Did not find doubledilla\n",
      "Did not find 50lb\n",
      "Did not find gifr\n",
      "Did not find vivel\n",
      "Did not find congratz\n",
      "Did not find negan\n",
      "Did not find belial\n",
      "Did not find 4'0\n",
      "Did not find hyderabadi\n",
      "Did not find manas\n",
      "Did not find 50s\n",
      "Did not find srinithi\n",
      "Did not find labour\n",
      "Did not find kids\r",
      "\r\n",
      "Did not find 'treasures'\n",
      "Did not find 'imposters'\n",
      "Did not find yummyyyyyy\n",
      "Did not find hobbes\n",
      "Did not find schumann\n",
      "Did not find 'papertowns'\n",
      "Did not find dimond\n",
      "Did not find 2onion\n",
      "Did not find mamare\n",
      "Did not find oooooooo\n",
      "Did not find comland\n",
      "Did not find clarita\n",
      "Did not find hodges\n",
      "Did not find dubois\n",
      "Did not find aode\n",
      "Did not find faira\n",
      "Did not find drydenas\n",
      "Did not find keurig\n",
      "Did not find gillette\n",
      "Did not find tarzan\n",
      "Did not find suunto\n",
      "Did not find tenction\n",
      "Did not find divya\n",
      "Did not find 2k17\n",
      "Did not find yoshis\n",
      "Did not find whataburger\n",
      "Did not find usain\n",
      "Did not find behaviours\n",
      "Did not find jj10\n",
      "Did not find 3way\n",
      "Did not find buca\n",
      "Did not find beppo\n",
      "Did not find winned\n",
      "Did not find 158km\n",
      "Did not find ingood\n",
      "Did not find peele\n",
      "Did not find 560\n",
      "Did not find rats'\n",
      "Did not find haills\n",
      "Did not find ptcb\n",
      "Did not find 70lbs\n",
      "Did not find dailly\n",
      "Did not find workplance\n",
      "Did not find kealakekua\n",
      "Did not find 13mp\n",
      "Did not find 61\n",
      "Did not find neobux\n",
      "Did not find bergman\n",
      "Did not find bergcat\n",
      "Did not find confptand\n",
      "Did not find vare\n",
      "Did not find 'homeland'\n",
      "Did not find \r",
      "\r",
      "\r",
      "\r\n",
      "Did not find pprice\n",
      "Did not find 'one\n",
      "Did not find man'\n",
      "Did not find weedwacked\n",
      "Did not find tenore\n",
      "Did not find 250cc\n",
      "Did not find 155\n",
      "Did not find inreresting\n",
      "Did not find facepack\n",
      "Did not find mcom\n",
      "Did not find controla\n",
      "Did not find kichadi\n",
      "Did not find anyoneas\n",
      "Did not find adonat\n",
      "Did not find alrighta\n",
      "Did not find 'ctrl\n",
      "Did not find snsd\n",
      "Did not find tirupathi\n",
      "Did not find venkateswara\n",
      "Did not find yercaud\n",
      "Did not find 12months\n",
      "Did not find kurinji\n",
      "Did not find deezer\n",
      "Did not find alfonso\n",
      "Did not find program\r",
      "\r\n",
      "Did not find alliexpres\n",
      "Did not find covera\n",
      "Did not find 74\n",
      "Did not find month\r",
      "\r\n",
      "Did not find takecare\n",
      "Did not find mahalaxshmi\n",
      "Did not find akinator\n",
      "Did not find 5mi\n",
      "Did not find xfinity\n",
      "Did not find watchathon\n",
      "Did not find happiness\r",
      "\r\n",
      "Did not find basundi\n",
      "Did not find basara\n",
      "Did not find makemytrip\n",
      "Did not find douluo\n",
      "Did not find dalu\n",
      "Did not find selvi\n",
      "Did not find jayalalitha\n",
      "Did not find ksa\n",
      "Did not find jovi\n",
      "Did not find aubrey\n",
      "Did not find eversing\n",
      "Did not find 'zipedee\n",
      "Did not find zipedee\n",
      "Did not find u18\n",
      "Did not find energised\n",
      "Did not find rocksmith\n",
      "Did not find habachi\n",
      "Did not find 5hour\n",
      "Did not find lkg\n",
      "Did not find chudidhars\n",
      "Did not find eternalenvy\n",
      "Did not find mcnuggets\n",
      "Did not find bogo\n",
      "Did not find choki\n",
      "Did not find carm\n",
      "Did not find 1250\n",
      "Did not find sute\n",
      "Did not find aeroplane\n",
      "Did not find fasr\n",
      "Did not find 10kms\n",
      "Did not find emilie\n",
      "Did not find manageri\n",
      "Did not find cafa\n",
      "Did not find lsa\n",
      "Did not find xj\n",
      "Did not find umass\n",
      "Did not find amherst\n",
      "Did not find happies\n",
      "Did not find fuckyourworriesla\n",
      "Did not find palpandi\n",
      "Did not find honourable\n",
      "Did not find neighbors'\n",
      "Did not find siva\n",
      "Did not find rainman\n",
      "Did not find 404\n",
      "Did not find 6'o\n",
      "Did not find manzeppi\n",
      "Did not find celldweller\n",
      "Did not find hear\r",
      "\r\n",
      "Did not find fantastics\n",
      "Did not find chefchaouen\n",
      "Did not find redford\n",
      "Did not find spacek\n",
      "Did not find ewell\n",
      "Did not find 1v1\n",
      "Did not find powermoves\n",
      "Did not find jerrys\n",
      "Did not find yellowstone\n",
      "Did not find pastier\n",
      "Did not find doubtfire\n",
      "Did not find indunal\n",
      "Did not find nrekasy4xzg\n",
      "Did not find bairava\n",
      "Did not find butterfinger\n",
      "Did not find shopping\r",
      "\r\n",
      "Did not find 320\n",
      "Did not find siddhivinayak\n",
      "Did not find jeevitha\n",
      "Did not find briedgroom\n",
      "Did not find sriramanavmi\n",
      "Did not find scred\n",
      "Did not find moment\r",
      "\r\n",
      "Did not find montenegro\n",
      "Did not find rollo\n",
      "Did not find sumesh\n",
      "Did not find pilay\n",
      "Did not find fillar\n",
      "Did not find basquiat\n",
      "Did not find rhett\n",
      "Did not find series'\n",
      "Did not find 71\n",
      "Did not find spartanburg\n",
      "Did not find squarepants\n",
      "Did not find gofundme\n",
      "Did not find peevious\n",
      "Did not find 31pts\n",
      "Did not find alaskan\n",
      "Did not find 214\n",
      "Did not find quatros\n",
      "Did not find carbondale\n",
      "Did not find 2ton\n",
      "Did not find feeded\n",
      "Did not find biscuts\n",
      "Did not find moter\n",
      "Did not find worker'\n",
      "Did not find subha\n",
      "Did not find verlander\n",
      "Did not find geetha\n",
      "Did not find dinner\r",
      "\r\n",
      "Did not find today\r",
      "\r\n",
      "Did not find jumprope\n",
      "Did not find liff\n",
      "Did not find gils\n",
      "Did not find cuddlig\n",
      "Did not find kraftwerk\n",
      "Did not find ecig\n",
      "Did not find salarya\n",
      "Did not find jibberjabber\n",
      "Did not find kimmy\n",
      "Did not find devonshire\n",
      "Did not find puding\n",
      "Did not find nanni\n",
      "Did not find turkers\n",
      "Did not find cooteas\n",
      "Did not find lorde\n",
      "Did not find marlboro\n",
      "Did not find nordstrom\n",
      "Did not find 230\n",
      "Did not find rodan\n",
      "Did not find fiestaware\n",
      "Did not find sleaping\n",
      "Did not find hualahli\n",
      "Did not find troubleshot\n",
      "Did not find kiyoshi\n",
      "Did not find ibukuro\n",
      "Did not find 78th\n",
      "Did not find droppedd\n",
      "Did not find amounam\n",
      "Did not find sammathama\n",
      "Did not find dinesh\n",
      "Did not find arkham\n",
      "Did not find blackgate\n",
      "Did not find bbq'd\n",
      "Did not find roscoes\n",
      "Did not find scify\n",
      "Did not find cappadocia\n",
      "Did not find emre\n",
      "Did not find tigers'\n",
      "Did not find kumbakonam\n",
      "Did not find varu\n",
      "Did not find haillth\n",
      "Did not find wyif\n",
      "Did not find screenshotted\n",
      "Did not find motoped\n",
      "Did not find modelling\n",
      "Did not find mooji\n",
      "Did not find barrowman\n",
      "Did not find frapp\n",
      "Did not find skyzone\n",
      "Did not find fxx\n",
      "Did not find theyave\n",
      "Did not find moments\r",
      "\r\n",
      "Did not find bagavthgeetha\n",
      "Did not find hotfixed\n",
      "Did not find tomomi\n",
      "Did not find kermit\n",
      "Did not find yldegar\n",
      "Did not find izombie\n",
      "Did not find 75lb\n",
      "Did not find wben\n",
      "Did not find ciderboys\n",
      "Did not find sita\n",
      "Did not find njoyed\n",
      "Did not find ite'\n",
      "Did not find ipsy\n",
      "Did not find totall\n",
      "Did not find maymont\n",
      "Did not find 00p\n",
      "Did not find woaah\n",
      "Did not find thrones0\n",
      "Did not find chillarn\n",
      "Did not find arenged\n",
      "Did not find sabres\n",
      "Did not find cauhgt\n",
      "Did not find 51\n",
      "Did not find preworkout\n",
      "Did not find gnc\n",
      "Did not find emdr\n",
      "Did not find behrain\n",
      "Did not find positif\n",
      "Did not find noosa\n",
      "Did not find hyvee\n",
      "Did not find gamefly\n",
      "Did not find katamaraidu\n",
      "Did not find hyundi\n",
      "Did not find morining\n",
      "Did not find centres\n",
      "Did not find silverado\n",
      "Did not find thirupathi\n",
      "Did not find 100pounds\n",
      "Did not find westworld\n",
      "Did not find imposters'\n",
      "Did not find epsom\n",
      "Did not find airconsole\n",
      "Did not find trilliax\n",
      "Did not find texed\n",
      "Did not find memaw\n",
      "Did not find chapple\n",
      "Did not find jeremiah\n",
      "Did not find bigelow\n",
      "Did not find barkbox\n",
      "Did not find bmv\n",
      "Did not find rickie\n",
      "Did not find dayfrom\n",
      "Did not find pizzahut\n",
      "Did not find 'pigeons\n",
      "Did not find krahapravecem\n",
      "Did not find pachira\n",
      "Did not find aquatica\n",
      "Did not find lnu9vtslmxo\n",
      "Did not find sheeran\n",
      "Did not find ashlee\n",
      "Did not find ftd\n",
      "Did not find ibprofen\n",
      "Did not find people'\n",
      "Did not find h1z1l\n",
      "Did not find esten\n",
      "Did not find magicans\n",
      "Did not find gokila\n",
      "Did not find offage\n",
      "Did not find appen\n",
      "Did not find nunes\n",
      "Did not find 'nother\n",
      "Did not find plaintails\n",
      "Did not find microframework\n",
      "Did not find 55000\n",
      "Did not find mohanlal\n",
      "Did not find listener'\n",
      "Did not find celibrated\n",
      "Did not find plant\r",
      "\r\n",
      "Did not find fairhope\n",
      "Did not find 'lazer\n",
      "Did not find flip'\n",
      "Did not find ejuice\n",
      "Did not find 20000\n",
      "Did not find yugioh\n",
      "Did not find ketchen\n",
      "Did not find jenner\n",
      "Did not find waikiki\n",
      "Did not find fewhours\n",
      "Did not find thirumoorthy\n",
      "Did not find ahappya\n",
      "Did not find iphone6\n",
      "Did not find sahar\n",
      "Did not find wmc\n",
      "Did not find 60s\n",
      "Did not find 40s\n",
      "Did not find siran\n",
      "Did not find 'likes\n",
      "Did not find firecamp\n",
      "Did not find drinked\n",
      "Did not find worshipping\n",
      "Did not find rs3000\n",
      "Did not find xboxone\n",
      "Did not find kanakadurga\n",
      "Did not find kineseology\n",
      "Did not find 'men\n",
      "Did not find honor'\n",
      "Did not find 'expert'\n",
      "Did not find wouldnat\n",
      "Did not find aget\n",
      "Did not find luckya\n",
      "Did not find alose\n",
      "Did not find paragraphers\n",
      "Did not find oaclock\n",
      "Did not find palkova\n",
      "Did not find rasokala\n",
      "Did not find toched\n",
      "Did not find portter\n",
      "Did not find 843\n",
      "Did not find miled\n",
      "Did not find sknic\n",
      "Did not find kadabra\n",
      "Did not find kamanaickanpatti\n",
      "Did not find hashim\n",
      "Did not find braggot\n",
      "Did not find bubbakoo\n",
      "Did not find 'hi'\n",
      "Did not find starbuck\n",
      "Did not find describel\n",
      "Did not find ebil\n",
      "Did not find vettel\n",
      "Did not find 200mph\n",
      "Did not find happifying\n",
      "Did not find bannana\n",
      "Did not find eded\n",
      "Did not find hawai'i\n",
      "Did not find kamen\n",
      "Did not find reieved\n",
      "Did not find flm\n",
      "Did not find wentworth\n",
      "Did not find tesol\n",
      "Did not find fudruckers\n",
      "Did not find piyo\n",
      "Did not find uji\n",
      "Did not find usmc\n",
      "Did not find osmonds\n",
      "Did not find smari\n",
      "Did not find conchords\n",
      "Did not find foid\n",
      "Did not find egift\n",
      "Did not find itall\n",
      "Did not find dayquil\n",
      "Did not find petrolium\n",
      "Did not find decresing\n",
      "Did not find hailee\n",
      "Did not find steinfeld\n",
      "Did not find 87th\n",
      "Did not find romio\n",
      "Did not find nextflix\n",
      "Did not find offiically\n",
      "Did not find sdad\n",
      "Did not find jigglers\n",
      "Did not find gegard\n",
      "Did not find mousasi\n",
      "Did not find zenzi\n",
      "Did not find foodworks\n",
      "Did not find parx\n",
      "Did not find hounddog\n",
      "Did not find timehop\n",
      "Did not find ao3\n",
      "Did not find 'just\n",
      "Did not find because'\n",
      "Did not find 9gag\n",
      "Did not find beverage'\n",
      "Did not find honeybun\n",
      "Did not find nationalised\n",
      "Did not find 100usd\n",
      "Did not find zao\n",
      "Did not find spermscope\n",
      "Did not find carnegie\n",
      "Did not find bugline\n",
      "Did not find hoers\n",
      "Did not find 18v\n",
      "Did not find blacky\n",
      "Did not find amiibo\n",
      "Did not find 'food\n",
      "Did not find face'\n",
      "Did not find ronny\n",
      "Did not find elkayam\n",
      "Did not find boaz\n",
      "Did not find adato\n",
      "Did not find eilon\n",
      "Did not find tirosh\n",
      "Did not find blumberg\n",
      "Did not find singtel\n",
      "Did not find innov8\n",
      "Did not find hintmachine\n",
      "Did not find centiment\n",
      "Did not find 67th\n",
      "Did not find strombolli\n",
      "Did not find clpp\n",
      "Did not find 787\n",
      "Did not find outkast\n",
      "Did not find ugk\n",
      "Did not find shemagh\n",
      "Did not find ariendal\n",
      "Did not find ramune\n",
      "Did not find redbulls\n",
      "Did not find ldr\n",
      "Did not find smallajust\n",
      "Did not find wokr\n",
      "Did not find drining\n",
      "Did not find haill\n",
      "Did not find firind\n",
      "Did not find gta5\n",
      "Did not find read'\n",
      "Did not find nordeman\n",
      "Did not find 0800\n",
      "Did not find 0600\n",
      "Did not find ucl\n",
      "Did not find 'cheat\n",
      "Did not find day'\n",
      "Did not find tyreke\n",
      "Did not find nowing\n",
      "Did not find lehigh\n",
      "Did not find 44\n",
      "Did not find finishg\n",
      "Did not find 9000\n",
      "Did not find adriatic\n",
      "Did not find hoidays\n",
      "Did not find yearas\n",
      "Did not find silentium\n",
      "Did not find snnd\n",
      "Did not find ssssssunday\n",
      "Did not find joshuua\n",
      "Did not find thunkable\n",
      "Did not find amell\n",
      "Did not find siskss\n",
      "Did not find patan\n",
      "Did not find petco\n",
      "Did not find angkor\n",
      "Did not find h1b\n",
      "Did not find trx\n",
      "Did not find masaniamman\n",
      "Did not find mookambika\n",
      "Did not find sr5\n",
      "Did not find enevnt\n",
      "Did not find hiolidays\n",
      "Did not find vishu\n",
      "Did not find 78\n",
      "Did not find 2004\n",
      "Did not find mazda6\n",
      "Did not find kerela\n",
      "Did not find 68th\n",
      "Did not find 'tilgul\n",
      "Did not find ghya\n",
      "Did not find bola'\n",
      "Did not find ungle\n",
      "Did not find ultasound\n",
      "Did not find stabilised\n",
      "Did not find bike\r",
      "\r\n",
      "Did not find germain\n",
      "Did not find dairi\n",
      "Did not find duplo\n",
      "Did not find real\r",
      "\r\n",
      "Did not find verginatize\n",
      "Did not find shit\r",
      "\r\n",
      "Did not find wahahahahah\n",
      "Did not find hangning\n",
      "Did not find puppie\n",
      "Did not find housesat\n",
      "Did not find sdr\n",
      "Did not find minkle\n",
      "Did not find santiago\n",
      "Did not find audubon\n",
      "Did not find nanjundeeshwarar\n",
      "Did not find flavours\n",
      "Did not find slytherin\n",
      "Did not find wonka\n",
      "Did not find pta\n",
      "Did not find aall\n",
      "Did not find finea\n",
      "Did not find cholar\n",
      "Did not find badge'\n",
      "Did not find americorps\n",
      "Did not find nco\n",
      "Did not find lottary\n",
      "Did not find 70as\n",
      "Did not find feb'\n",
      "Did not find guggenheim\n",
      "Did not find 1750\n",
      "Did not find patong\n",
      "Did not find singapore\r",
      "\r\n",
      "Did not find fantasmic\n",
      "Did not find bodeguita\n",
      "Did not find unnecto\n",
      "Did not find cwu\n",
      "Did not find hannibal\n",
      "Did not find shajitha\n",
      "Did not find roblox\n",
      "Did not find 06\n",
      "Did not find 38871\n",
      "Did not find nandi\n",
      "Did not find myat\n",
      "Did not find khine\n",
      "Did not find thumbelina\n",
      "Did not find juiz\n",
      "Did not find patricksa\n",
      "Did not find happeneded\n",
      "Did not find mbbs\n",
      "Did not find convecation\n",
      "Did not find faar\n",
      "Did not find chuy\n",
      "Did not find deville\n",
      "Did not find spektor\n",
      "Did not find 'abdullah'\n",
      "Did not find dranked\n",
      "Did not find sunrisers\n",
      "Did not find 736\n",
      "Did not find gpp\n",
      "Did not find mathjax\n",
      "Did not find trivandrum\n",
      "Did not find lookg\n",
      "Did not find c2e2\n",
      "Did not find 1v6\n",
      "Did not find rables\n",
      "Did not find astoria\n",
      "Did not find momoa\n",
      "Did not find her\r",
      "\r\n",
      "Did not find haddie\n",
      "Did not find losingggggg\n",
      "Did not find ghis\n",
      "Did not find atlatna\n",
      "Did not find sevilla\n",
      "Did not find himbut\n",
      "Did not find iwant\n",
      "Did not find grame\n",
      "Did not find sigur\n",
      "Did not find dogsitting\n",
      "Did not find maghub\n",
      "Did not find gameflip\n",
      "Did not find catan\n",
      "Did not find cleburne\n",
      "Did not find acknowledgement\n",
      "Did not find mca\n",
      "Did not find puget\n",
      "Did not find 10lb\n",
      "Did not find canfield\n",
      "Did not find tulpas\n",
      "Did not find lohri\n",
      "Did not find 'overflowing'\n",
      "Did not find intsalled\n",
      "Did not find oahu\n",
      "Did not find saris'\n",
      "Did not find ps3000\n",
      "Did not find aryabhatta\n",
      "Did not find oldtimecandy\n",
      "Did not find sorento\n",
      "Did not find schwarber\n",
      "Did not find legoland\n",
      "Did not find 34th\n",
      "Did not find bohol\n",
      "Did not find j7\n",
      "Did not find forca\n",
      "Did not find 'all\n",
      "Did not find rbc\n",
      "Did not find wbc\n",
      "Did not find pilgrimed\n",
      "Did not find kamanayakkanpatti\n",
      "Did not find onam\n",
      "Did not find shimla\n",
      "Did not find frutti\n",
      "Did not find creame\n",
      "Did not find world'\n",
      "Did not find guruvayur\n",
      "Did not find tropicarium\n",
      "Did not find rerady\n",
      "Did not find antincipating\n",
      "Did not find 11lbs\n",
      "Did not find experience\r",
      "\r\n",
      "Did not find hoas\n",
      "Did not find olympique\n",
      "Did not find marseille\n",
      "Did not find liliana\n",
      "Did not find carama\n",
      "Did not find herea\n",
      "Did not find changesa\n",
      "Did not find backa\n",
      "Did not find comea\n",
      "Did not find crya\n",
      "Did not find higha\n",
      "Did not find waya\n",
      "Did not find kross\n",
      "Did not find repoed\n",
      "Did not find nany\n",
      "Did not find 89th\n",
      "Did not find i''ve\n",
      "Did not find tiramissu\n",
      "Did not find rotc\n",
      "Did not find jeans'\n",
      "Did not find bajji\n",
      "Did not find adequan\n",
      "Did not find capabilities\r",
      "\r\n",
      "Did not find playmobil\n",
      "Did not find applecare\n",
      "Did not find reblogged\n",
      "Did not find everyda\n",
      "Did not find ogs\n",
      "Did not find alogn\n",
      "Did not find burdwan\n",
      "Did not find gerogia\n",
      "Did not find kappad\n",
      "Did not find boracay\n",
      "Did not find polymorous\n",
      "Did not find aps10\n",
      "Did not find philz\n",
      "Did not find infiniti\n",
      "Did not find facetimes\n",
      "Did not find 76ers\n",
      "Did not find sharehouse\n",
      "Did not find virat\n",
      "Did not find kohli\n",
      "Did not find 51st\n",
      "Did not find 1911\n",
      "Did not find seaworld\n",
      "Did not find 69\n",
      "Did not find dsce\n",
      "Did not find intranced\n",
      "Did not find punal\n",
      "Did not find leck\n",
      "Did not find venesuela\n",
      "Did not find recognised\n",
      "Did not find trumbo\n",
      "Did not find meeka\n",
      "Did not find gates'\n",
      "Did not find incredible\r",
      "\r\n",
      "Did not find smithsonian\n",
      "Did not find uturn\n",
      "Did not find hatchimal\n",
      "Did not find seinfeld\n",
      "Did not find dicouraging\n",
      "Did not find teps\n",
      "Did not find 'unload'\n",
      "Did not find leavel\n",
      "Did not find yorke\n",
      "Did not find cassini\n",
      "Did not find huygens\n",
      "Did not find torino\n",
      "Did not find wfh\n",
      "Did not find matetial\n",
      "Did not find vaper\n",
      "Did not find listeing\n",
      "Did not find 4500\n",
      "Did not find pismo\n",
      "Did not find komboucha\n",
      "Did not find pluralsight\n",
      "Did not find gretzky\n",
      "Did not find neda\n",
      "Did not find eacher\n",
      "Did not find staing\n",
      "Did not find kilda\n",
      "Did not find mohegan\n",
      "Did not find decor\r",
      "\r\n",
      "Did not find williamsburg\n",
      "Did not find kdrama\n",
      "Did not find itcing\n",
      "Did not find puality\n",
      "Did not find autozone\n",
      "Did not find 06th\n",
      "Did not find 80as\n",
      "Did not find 90as\n",
      "Did not find sharetea\n",
      "Did not find mom\r",
      "\r\n",
      "Did not find maldives\n",
      "Did not find reciepe\n",
      "Did not find 59th\n",
      "Did not find seiko\n",
      "Did not find becane\n",
      "Did not find recntly\n",
      "Did not find kilometre\n",
      "Did not find unger\n",
      "Did not find munster\n",
      "Did not find venkatesha\n",
      "Did not find refugee'\n",
      "Did not find arduino\n",
      "Did not find daaa\n",
      "Did not find sadies\n",
      "Did not find tranquillity\n",
      "Did not find rutger\n",
      "Did not find msw\n",
      "Did not find alayna\n",
      "Did not find plannes\n",
      "Did not find mindstate\n",
      "Did not find irusqm5zskq\n",
      "Did not find sicily\n",
      "Did not find 02\n",
      "Did not find weedwacker\n",
      "Did not find dcss\n",
      "Did not find navaratri\n",
      "Did not find mcrib\n",
      "Did not find mz\n",
      "Did not find mewtoo\n",
      "Did not find araku\n",
      "Did not find frap\n",
      "Did not find 12k\n",
      "Did not find gurpreet\n",
      "Did not find awhie\n",
      "Did not find thiruthani\n",
      "Did not find yachty\n",
      "Did not find dependant\n",
      "Did not find myt\n",
      "Did not find yohooo\n",
      "Did not find woohoooo\n",
      "Did not find behaviour\n",
      "Did not find auro\n",
      "Did not find ammu\n",
      "Did not find wlak\n",
      "Did not find hatfield\n",
      "Did not find succses\n",
      "Did not find quiplash\n",
      "Did not find 13yrs\n",
      "Did not find gtr\n",
      "Did not find 2007\n",
      "Did not find niel\n",
      "Did not find gorsuch\n",
      "Did not find scotus\n",
      "Did not find cosplays\n",
      "Did not find songkran\n",
      "Did not find allentown\n",
      "Did not find vivaldi\n",
      "Did not find buttflap\n",
      "Did not find blackshear\n",
      "Did not find that\r",
      "\r\n",
      "Did not find 15000\n",
      "Did not find geoffrey\n",
      "Did not find wsh\n",
      "Did not find agastyaarkodam\n",
      "Did not find yangon\n",
      "Did not find pocatello\n",
      "Did not find veia\n",
      "Did not find biug\n",
      "Did not find balcon\n",
      "Did not find g3po\n",
      "Did not find marieh\n",
      "Did not find tamanny\n",
      "Did not find 25c\n",
      "Did not find 19th\n",
      "Did not find 306\n",
      "Did not find vib\n",
      "Did not find behavioural\n",
      "Did not find nyp\n",
      "Did not find 100s\n",
      "Did not find pipestem\n",
      "Did not find zambreno\n",
      "Did not find bnb'b\n",
      "Did not find phelp\n",
      "Did not find v20\n",
      "Did not find 270\n",
      "Did not find quabbin\n",
      "Did not find mazdaspeed\n",
      "Did not find 1yo\n",
      "Did not find animaniacs\n",
      "Did not find oates\n",
      "Did not find ibd\n",
      "Did not find statted\n",
      "Did not find portgual\n",
      "Did not find viisting\n",
      "Did not find spinergy\n",
      "Did not find 75th\n",
      "Did not find ggot\n",
      "Did not find sparthenian\n",
      "Did not find wango\n",
      "Did not find celerabrating\n",
      "Did not find 1650\n",
      "Did not find bahubali2\n",
      "Did not find bassnectar\n",
      "Did not find motheras\n",
      "Did not find multple\n",
      "Did not find bhahubali2\n",
      "Did not find 6ou\n",
      "Did not find gokarting\n",
      "Did not find fridently\n",
      "Did not find jomin\n",
      "Did not find spenting\n",
      "Did not find 'hurray'\n",
      "Did not find corpio'\n",
      "Did not find 'nicco\n",
      "Did not find park'\n",
      "Did not find triking\n",
      "Did not find cars'\n",
      "Did not find 'giant\n",
      "Did not find wheel'\n",
      "Did not find coaster'\n",
      "Did not find plash'\n",
      "Did not find tructions\n",
      "Did not find 7is\n",
      "Did not find travelld\n",
      "Did not find maghan\n",
      "Did not find zoey\n",
      "Did not find naac\n",
      "Did not find sonil\n",
      "Did not find 12yrs\n",
      "Did not find salery\n",
      "Did not find nielsen\n",
      "Did not find bogor\n",
      "Did not find seeings\n",
      "Did not find anastasio\n",
      "Did not find coulndt\n",
      "Did not find deepend\n",
      "Did not find burlington\n",
      "Did not find residents'\n",
      "Did not find 'vincent\n",
      "Did not find giften\n",
      "Did not find daughters'\n",
      "Did not find itil\n",
      "Did not find eveyday\n",
      "Did not find 59\n",
      "Did not find 1920\n",
      "Did not find 1920s\n",
      "Did not find rajini\n",
      "Did not find kanth\n",
      "Did not find villabous\n",
      "Did not find disgnosed\n",
      "Did not find disconfirmed\n",
      "Did not find swagbucks\n",
      "Did not find 5740\n",
      "Did not find jedis\n",
      "Did not find rtx\n",
      "Did not find campi\n",
      "Did not find bfg\n",
      "Did not find 500th\n",
      "Did not find cito\n",
      "Did not find depeche\n",
      "Did not find garand\n",
      "Did not find xbox1\n",
      "Did not find warbotron\n",
      "Did not find bruticus\n",
      "Did not find 3200\n",
      "Did not find comiccon\n",
      "Did not find grisham\n",
      "Did not find describeda\n",
      "Did not find feburary\n",
      "Did not find 81st\n",
      "Did not find 30point\n",
      "Did not find krauss\n",
      "Did not find truman\n",
      "Did not find ptm\n",
      "Did not find m6\n",
      "Did not find eloctution\n",
      "Did not find 'unicorn\n",
      "Did not find foose\n",
      "Did not find loveland\n",
      "Did not find baileys\n",
      "Did not find whatas\n",
      "Did not find spelt\n",
      "Did not find paradeawithout\n",
      "Did not find insas\n",
      "Did not find driveras\n",
      "Did not find awhether\n",
      "Did not find inconspicuousness\n",
      "Did not find akashmiris\n",
      "Did not find athereas\n",
      "Did not find athese\n",
      "Did not find milaapa\n",
      "Did not find shopian\n",
      "Did not find afeem\n",
      "Did not find asaara\n",
      "Did not find wahaan\n",
      "Did not find apahalgam\n",
      "Did not find kilometres\n",
      "Did not find akesar\n",
      "Did not find valley'\n",
      "Did not find kahwah\n",
      "Did not find asalaam\n",
      "Did not find ailakum\n",
      "Did not find aour\n",
      "Did not find atry\n",
      "Did not find iskan\n",
      "Did not find sarrow\n",
      "Did not find sorroundling\n",
      "Did not find pnw\n",
      "Did not find 837\n",
      "Did not find 2weeks\n",
      "Did not find tambola\n",
      "Did not find sisters'\n",
      "Did not find zoya\n",
      "Did not find sawatch\n",
      "Did not find sathya\n",
      "Did not find 'medicine'\n",
      "Did not find carvel\n",
      "Did not find trubisky\n",
      "Did not find 200km\n",
      "Did not find springfest\n",
      "Did not find venmo\n",
      "Did not find 90f\n",
      "Did not find calls\r",
      "\r\n",
      "Did not find cicis\n",
      "Did not find darksouls\n",
      "Did not find pikachu\n",
      "Did not find cronuts\n",
      "Did not find frida\n",
      "Did not find aristotle\n",
      "Did not find 76\n",
      "Did not find eat24\n",
      "Did not find heinekens\n",
      "Did not find meijers'\n",
      "Did not find gotham\n",
      "Did not find netflixs\n",
      "Did not find myh\n",
      "Did not find roseacea\n",
      "Did not find 80lbs\n",
      "Did not find 12lbs\n",
      "Did not find 508\n",
      "Did not find 318\n",
      "Did not find h3h3\n",
      "Did not find anova\n",
      "Did not find chocolatea\n",
      "Did not find yvette\n",
      "Did not find aliexpress\n",
      "Did not find frappacino\n",
      "Did not find exam473\n",
      "Did not find collegemates\n",
      "Did not find abidaia\n",
      "Did not find ceremonya\n",
      "Did not find samie\n",
      "Did not find now\r",
      "\r\n",
      "Did not find feeings\n",
      "Did not find 17febuary\n",
      "Did not find padmanabhaswamy\n",
      "Did not find cinipriya\n",
      "Did not find kattappa\n",
      "Did not find donalds\n",
      "Did not find adhwik\n",
      "Did not find howlite\n",
      "Did not find cappacino\n",
      "Did not find enounced\n",
      "Did not find teal'\n",
      "Did not find 'will'\n",
      "Did not find grandparents'\n",
      "Did not find abdillah\n",
      "Did not find thinkgeek\n",
      "Did not find monthsary\n",
      "Did not find ozarks\n",
      "Did not find clienst\n",
      "Did not find oft'\n",
      "Did not find springsteen\n",
      "Did not find lemonis\n",
      "Did not find bowiling\n",
      "Did not find bingewatching\n",
      "Did not find headech\n",
      "Did not find 1100\n",
      "Did not find sodastream\n",
      "Did not find gml\n",
      "Did not find kauai\n",
      "Did not find myx\n",
      "Did not find feederweb\n",
      "Did not find courson\n",
      "Did not find kesha\n",
      "Did not find 110ug\n",
      "Did not find arbys\n",
      "Did not find galentine\n",
      "Did not find 1700\n",
      "Did not find daye\n",
      "Did not find appplying\n",
      "Did not find ti3\n",
      "Did not find dota2\n",
      "Did not find natus\n",
      "Did not find vincere\n",
      "Did not find thenmalai\n",
      "Did not find thiruvananthapuram\n",
      "Did not find auncle\n",
      "Did not find tomas\n",
      "Did not find episode\r",
      "\r\n",
      "Did not find lotro\n",
      "Did not find usmle\n",
      "Did not find realising\n",
      "Did not find lifeas\n",
      "Did not find matthieu\n",
      "Did not find ricard\n",
      "Did not find coldplay\n",
      "Did not find bartlett\n",
      "Did not find cbr250\n",
      "Did not find jquery\n",
      "Did not find annyang\n",
      "Did not find k4\n",
      "Did not find volcom\n",
      "Did not find winkies\n",
      "Did not find quadlings\n",
      "Did not find gillikins\n",
      "Did not find cat\r",
      "\r\n",
      "Did not find irene\n",
      "Did not find kunnur\n",
      "Did not find 'aw\n",
      "Did not find 1995\n",
      "Did not find tabeltop\n",
      "Did not find 'november\n",
      "Did not find freedom'\n",
      "Did not find favoured\n",
      "Did not find pay\r",
      "\r\n",
      "Did not find osheaga\n",
      "Did not find krosus\n",
      "Did not find leaved'\n",
      "Did not find iui\n",
      "Did not find 20lbs\n",
      "Did not find 33rd\n",
      "Did not find carnevale\n",
      "Did not find trawberry\n",
      "Did not find 1000th\n",
      "Did not find '70\n",
      "Did not find '30\n",
      "Did not find '40\n",
      "Did not find dollywood\n",
      "Did not find gump\n",
      "Did not find emojis\n",
      "Did not find aqours\n",
      "Did not find hirshhorn\n",
      "Did not find yoho\n",
      "Did not find 11am\n",
      "Did not find bumbo\n",
      "Did not find mayot\n",
      "Did not find raynauds\n",
      "Did not find 225lbs\n",
      "Did not find belgrade\n",
      "Did not find intermediat\n",
      "Did not find folsom\n",
      "Did not find photo'\n",
      "Did not find 95th\n",
      "Did not find sikar\n",
      "Did not find jead\n",
      "Did not find whetstones\n",
      "Did not find 100th\n",
      "Did not find raisea\n",
      "Did not find tuscaloosa\n",
      "Did not find 100o\n",
      "Did not find yully\n",
      "Did not find baji\n",
      "Did not find redmi\n",
      "Did not find 45th\n",
      "Did not find cury\n",
      "Did not find hubbyas\n",
      "Did not find sandip\n",
      "Did not find nabami\n",
      "Did not find mittal\n",
      "Did not find 57\n",
      "Did not find aadat\n",
      "Did not find majbur\n",
      "Did not find athanksa\n",
      "Did not find sheegra\n",
      "Did not find hanumanji\n",
      "Did not find visit\r",
      "\r\n",
      "Did not find wqlk\n",
      "Did not find abcs\n",
      "Did not find 'mommy'\n",
      "Did not find rememberable\n",
      "Did not find gamgam\n",
      "Did not find cancer\r",
      "\r\n",
      "Did not find longbeach\n",
      "Did not find opoid\n",
      "Did not find chevelle\n",
      "Did not find krispie\n",
      "Did not find maraton\n",
      "Did not find thottapetta\n",
      "Did not find weoght\n",
      "Did not find 'vodka'\n",
      "Did not find roseanne\n",
      "Did not find takl\n",
      "Did not find pakupali\n",
      "Did not find iive\n",
      "Did not find 'violent\n",
      "Did not find argument'\n",
      "Did not find 'lady\n",
      "Did not find 'commitment'\n",
      "Did not find rupaul\n",
      "Did not find azores\n",
      "Did not find reshma\n",
      "Did not find danelectro\n",
      "Did not find bloons\n",
      "Did not find runyon\n",
      "Did not find lularoe\n",
      "Did not find coote\n",
      "Did not find diagon\n",
      "Did not find machang\n",
      "Did not find lonavla\n",
      "Did not find supposd\n",
      "Did not find snohomish\n",
      "Did not find conferrence\n",
      "Did not find 254th\n",
      "Did not find eamcet\n",
      "Did not find masssave\n",
      "Did not find magicbands\n",
      "Did not find cabulance\n",
      "Did not find shittily\n",
      "Did not find maury\n",
      "Did not find 2107\n",
      "Did not find bufferas\n",
      "Did not find slidedeck\n",
      "Did not find nutureshock\n",
      "Did not find merryman\n",
      "Did not find acancer\n",
      "Did not find asunshinea\n",
      "Did not find abasket\n",
      "Did not find customersa\n",
      "Did not find woodford\n",
      "Did not find altima\n",
      "Did not find 215\n",
      "Did not find fighted\n",
      "Did not find preassure\n",
      "Did not find cindrella\n",
      "Did not find iobit\n",
      "Did not find 150cc\n",
      "Did not find sweing\n",
      "Did not find jeasus\n",
      "Did not find aakshayatriya\n",
      "Did not find 'god\n",
      "Did not find blindman\n",
      "Did not find mandovi\n",
      "Did not find alberquerue\n",
      "Did not find comtrol\n",
      "Did not find mushroomhead\n",
      "Did not find aqes\n",
      "Did not find coakers\n",
      "Did not find sphaghetti\n",
      "Did not find gaurdians\n",
      "Did not find saturnz\n",
      "Did not find barz\n",
      "Did not find 'baby'\n",
      "Did not find gettijg\n",
      "Did not find appearance\r",
      "\r\n",
      "Did not find albania\n",
      "Did not find brockmire\n",
      "Did not find budritas\n",
      "Did not find 428i\n",
      "Did not find picknick\n",
      "Did not find liquidsky\n",
      "Did not find jarrad\n",
      "Did not find omegle\n",
      "Did not find 2reply\n",
      "Did not find ciroc\n",
      "Did not find 'overrule'\n",
      "Did not find vallejo\n",
      "Did not find seti\n",
      "Did not find oubre\n",
      "Did not find olynek\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not find foyt\n",
      "Did not find 1983\n",
      "Did not find calistoga\n",
      "Did not find ramsay\n",
      "Did not find iworked\n",
      "Did not find april'17\n",
      "Did not find 205\n",
      "Did not find hayes'\n",
      "Did not find dooney\n",
      "Did not find freeleech\n",
      "Did not find deocrations\n",
      "Did not find spikeball\n",
      "Did not find 2600\n",
      "Did not find 2020\n",
      "Did not find momas\n",
      "Did not find rpcied\n",
      "Did not find finalised\n",
      "Did not find jillian\n",
      "Did not find 325\n",
      "Did not find emmanuel\n",
      "Did not find camgirl\n",
      "Did not find witg\n",
      "Did not find hard\r",
      "\r\n",
      "Did not find aeropostale\n",
      "Did not find joplin\n",
      "Did not find caspian\n",
      "Did not find aileen\n",
      "Did not find diabilities\n",
      "Did not find matlock\n",
      "Did not find bhagat\n",
      "Did not find heroas\n",
      "Did not find qol\n",
      "Did not find happiness'\n",
      "Did not find erasmus\n",
      "Did not find rotterdam\n",
      "Did not find thirumala\n",
      "Did not find dyi\n",
      "Did not find mendocino\n",
      "Did not find peoms\n",
      "Did not find peom\n",
      "Did not find dantdm\n",
      "Did not find 9month\n",
      "Did not find iep\n",
      "Did not find cartain\n",
      "Did not find listeining\n",
      "Did not find ppppotty\n",
      "Did not find attendees'\n",
      "Did not find imgur\n",
      "Did not find acustom\n",
      "Did not find chacos\n",
      "Did not find 22yrs\n",
      "Did not find shihtzu\n",
      "Did not find ignature\n",
      "Did not find recipe'\n",
      "Did not find waubonsee\n",
      "Did not find nonary\n",
      "Did not find afterpart\n",
      "Did not find cozumel\n",
      "Did not find honduras\n",
      "Did not find sathuragiri\n",
      "Did not find spemt\n",
      "Did not find fronter\n",
      "Did not find vba\n",
      "Did not find clamcakes\n",
      "Did not find 'be\n",
      "Did not find '\r",
      "\r\n",
      "Did not find 670\n",
      "Did not find gymnstics\n",
      "Did not find shadowrun\n",
      "Did not find hdi\n",
      "Did not find gejang\n",
      "Did not find rooomate\n",
      "Did not find realzing\n",
      "Did not find caughing\n",
      "Did not find energitic\n",
      "Did not find easton\n",
      "Did not find puducherry\n",
      "Did not find circe\n",
      "Did not find vishal\n",
      "Did not find 07\n",
      "Did not find 106\n",
      "Did not find preposed\n",
      "Did not find incence\n",
      "Did not find 'orange\n",
      "Did not find black'\n",
      "Did not find l'elephant\n",
      "Did not find inawhile\n",
      "Did not find fiverr\n",
      "Did not find estuve\n",
      "Did not find charlando\n",
      "Did not find rato\n",
      "Did not find adolescente\n",
      "Did not find adema\n",
      "Did not find contarme\n",
      "Did not find hablamos\n",
      "Did not find pelaculas\n",
      "Did not find leadas\n",
      "Did not find intercambiamos\n",
      "Did not find opiniones\n",
      "Did not find unforgettable\r",
      "\r\n",
      "Did not find enquired\n",
      "Did not find 'fire\n",
      "Did not find exactly\r",
      "\r\n",
      "Did not find trifexis\n",
      "Did not find eurotrip\n",
      "Did not find cashcrate\n",
      "Did not find piligrim\n",
      "Did not find 09\n",
      "Did not find singampunari\n",
      "Did not find bryants\n",
      "Did not find nake\n",
      "Did not find activty\n",
      "Did not find mummyas\n",
      "Did not find csats\n",
      "Did not find 6400\n",
      "Did not find sdl\n",
      "Did not find levias\n",
      "Did not find frappachinos\n",
      "Did not find sense8\n",
      "Did not find helena\n",
      "Did not find dccc\n",
      "Did not find gundam\n",
      "Did not find 39th\n",
      "Did not find ibotta\n",
      "Did not find daazs\n",
      "Did not find hayward\n",
      "Did not find mulan\n",
      "Did not find 100000\n",
      "Did not find shevaroys\n",
      "Did not find rockville\n",
      "Did not find 'diary\n",
      "Did not find kid'\n",
      "Did not find floot\n",
      "Did not find 1sr\n",
      "Did not find miyazaki\n",
      "Did not find picross\n",
      "Did not find trecking\n",
      "Did not find narasimhaswami\n",
      "Did not find tirunalla\n",
      "Did not find houes\n",
      "Did not find nephews'\n",
      "Did not find 'me\n",
      "Did not find time'\n",
      "Did not find 20minute\n",
      "Did not find wellcare\n",
      "Did not find 92nd\n",
      "Did not find 1800\n",
      "Did not find xanth\n",
      "Did not find walkinging\n",
      "Did not find cince\n",
      "Did not find days'\n",
      "Did not find bladerunner\n",
      "Did not find 2049\n",
      "Did not find scannings\n",
      "Did not find 'pineapple'\n",
      "Did not find minpin\n",
      "Did not find sweedish\n",
      "Did not find hersey\n",
      "Did not find sheetz\n",
      "Did not find goseigen\n",
      "Did not find out\r",
      "\r\n",
      "Did not find raddison\n",
      "Did not find amway\n",
      "Did not find brussel\n",
      "Did not find reasult\n",
      "Did not find 12\r",
      "\r\n",
      "Did not find meeat\n",
      "Did not find exciste\n",
      "Did not find staminas\n",
      "Did not find 65th\n",
      "Did not find 88th\n",
      "Did not find majora\n",
      "Did not find mccafe\n",
      "Did not find guradians\n",
      "Did not find karenina\n",
      "Did not find swateek\n",
      "Did not find 'rail\n",
      "Did not find trail'\n",
      "Did not find icici\n",
      "Did not find proposel\n",
      "Did not find 102nd\n",
      "Did not find lekum\n",
      "Did not find jonthan\n",
      "Did not find coulton\n",
      "Did not find still\r",
      "\r\n",
      "Did not find familyes\n",
      "Did not find srisailam\n",
      "Did not find hotal\n",
      "Did not find laputa\n",
      "Did not find ghibli\n",
      "Did not find 15years\n",
      "Did not find ayou\n",
      "Did not find cosin\n",
      "Did not find simla\n",
      "Did not find himachal\n",
      "Did not find 44th\n",
      "Did not find 'goodnight\n",
      "Did not find wrc\n",
      "Did not find dlsu\n",
      "Did not find joyerz\n",
      "Did not find 'worship\n",
      "Did not find team'\n",
      "Did not find 71st\n",
      "Did not find tspring\n",
      "Did not find ravelry\n",
      "Did not find gernerally\n",
      "Did not find assistising\n",
      "Did not find nclex\n",
      "Did not find 850\n",
      "Did not find 992\n",
      "Did not find beale\n",
      "Did not find meiji\n",
      "Did not find 'parent\n",
      "Did not find date'\n",
      "Did not find thishi\n",
      "Did not find mignogna\n",
      "Did not find abook\n",
      "Did not find 405\n",
      "Did not find nicu\n",
      "Did not find lcsw\n",
      "Did not find 'okay'\n",
      "Did not find 3000cp\n",
      "Did not find thorntons\n",
      "Did not find 30s\n",
      "Did not find uncc\n",
      "Did not find mordor\n",
      "Did not find keffir\n",
      "Did not find nevgative\n",
      "Did not find byoc\n",
      "Did not find quakecon\n",
      "Did not find '13\n",
      "Did not find why'\n",
      "Did not find valenties\n",
      "Did not find bananna\n",
      "Did not find porco\n",
      "Did not find dualshock\n",
      "Did not find emulate\r",
      "\r\n",
      "Did not find smaug\n",
      "Did not find jinga\n",
      "Did not find dlorida\n",
      "Did not find imperiale\n",
      "Did not find levis\n",
      "Did not find barkley\n",
      "Did not find 565\n",
      "Did not find cochin\n",
      "Did not find 110km\n",
      "Did not find alleppey\n",
      "Did not find 140km\n",
      "Did not find 04869\n",
      "Did not find 222014\n",
      "Did not find lakepalacethekkady\n",
      "Did not find gaved\n",
      "Did not find enjoed\n",
      "Did not find remington\n",
      "Did not find 870\n",
      "Did not find teespring\n",
      "Did not find 'tickling'\n",
      "Did not find mcnugget\n",
      "Did not find ccl\n",
      "Did not find rooed\n",
      "Did not find velankanni\n",
      "Did not find nagapattinam\n",
      "Did not find mturkers\n",
      "Did not find gruffs\n",
      "Did not find retropie\n",
      "Did not find frisco\n",
      "Did not find sbarro\n",
      "Did not find norweigan\n",
      "Did not find moonglow\n",
      "Did not find chabon\n",
      "Did not find warcrafft\n",
      "Did not find phadera\n",
      "Did not find repice\n",
      "Did not find bagubali\n",
      "Did not find sonuttaa\n",
      "Did not find fulfil\n",
      "Did not find himalaya\n",
      "Did not find valtteri\n",
      "Did not find bottas\n",
      "Did not find pinson\n",
      "Did not find kneww\n",
      "Did not find wallenpaupack\n",
      "Did not find valleyfair\n",
      "Did not find baghubali\n",
      "Did not find maarten\n",
      "Did not find daz\n",
      "Did not find smooshy\n",
      "Did not find rhode\n",
      "Did not find swimmies\n",
      "Did not find remarkedly\n",
      "Did not find mcdouble\n",
      "Did not find overlock\n",
      "Did not find workiing\n",
      "Did not find kawhi\n",
      "Did not find wimmer'\n",
      "Did not find ralphs\n",
      "Did not find frappuchinos\n",
      "Did not find fabiola\n",
      "Did not find willeford\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2687"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index = gensim.models.KeyedVectors.load_word2vec_format('D:/Datasets/embeddings/Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print(VOCAB_SIZE)\n",
    "\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, 300))\n",
    "count = 0\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = embeddings_index[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        print(f'Did not find {word}')\n",
    "        count += 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20810, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>24h</th>\n",
       "      <th>3m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27673</td>\n",
       "      <td>[1, 21, 16, 3, 762, 314, 11, 282, 1, 90, 12593...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27674</td>\n",
       "      <td>[1, 7, 12, 22, 2, 62, 17, 2277, 1423, 10, 80, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27675</td>\n",
       "      <td>[1, 21, 4, 6, 393, 37, 91, 5, 101, 929, 0, 0, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27676</td>\n",
       "      <td>[25, 19, 3, 1615, 312, 11, 46, 48, 13, 4418, 1...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27677</td>\n",
       "      <td>[1, 21, 11, 1903, 4, 3801, 4198, 20, 12595, 12...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid                                             tokens   24h     3m\n",
       "0  27673  [1, 21, 16, 3, 762, 314, 11, 282, 1, 90, 12593...  True  False\n",
       "1  27674  [1, 7, 12, 22, 2, 62, 17, 2277, 1423, 10, 80, ...  True  False\n",
       "2  27675  [1, 21, 4, 6, 393, 37, 91, 5, 101, 929, 0, 0, ...  True  False\n",
       "3  27676  [25, 19, 3, 1615, 312, 11, 46, 48, 13, 4418, 1...  True  False\n",
       "4  27677  [1, 21, 11, 1903, 4, 3801, 4198, 20, 12595, 12...  True  False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['24h'] = df.reflection_period == '24h'\n",
    "df['3m'] = df.reflection_period == '3m'\n",
    "df.drop(['reflection_period'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60321, 4) 60321\n"
     ]
    }
   ],
   "source": [
    "print(df.shape, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60321\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "labels_to_cats = {'achievement':      (1, 0, 0, 0, 0, 0, 0),\n",
    "                  'affection':        (0, 1, 0, 0, 0, 0, 0),\n",
    "                  'enjoy_the_moment': (0, 0, 1, 0, 0, 0, 0),\n",
    "                  'nature':           (0, 0, 0, 1, 0, 0, 0),\n",
    "                  'exercise':         (0, 0, 0, 0, 1, 0, 0),\n",
    "                  'bonding':          (0, 0, 0, 0, 0, 1, 0),\n",
    "                  'leisure':          (0, 0, 0, 0, 0, 0, 1)}\n",
    "\n",
    "cats_to_labels = dict()\n",
    "for k, v in labels_to_cats.items():\n",
    "    cats_to_labels[v] = k\n",
    "\n",
    "y = []\n",
    "for label in labels:\n",
    "    y.append(labels_to_cats[label])\n",
    "\n",
    "y = np.array(y)\n",
    "print(len(y))\n",
    "print(len(df.tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle():\n",
    "    x_train, x_val, y_train, y_val = train_test_split(df, y, test_size=0.2)\n",
    "    x_train_24h = x_train['24h']\n",
    "    x_train_3m = x_train['3m']\n",
    "    x_val_24h = x_val['24h']\n",
    "    x_val_3m = x_val['3m']\n",
    "    x_train_new = []\n",
    "    for element in x_train.tokens:\n",
    "        x_train_new.append(np.array(element))\n",
    "    x_train_new = np.array(x_train_new)\n",
    "\n",
    "    x_val_new = []\n",
    "    for element in x_val.tokens:\n",
    "        x_val_new.append(np.array(element))\n",
    "    x_val_new = np.array(x_val_new)\n",
    "\n",
    "    return x_train_new, x_val_new, x_train_24h, x_train_3m, x_val_24h, x_val_3m, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new, x_val_new, x_train_24h, x_train_3m, x_val_24h, x_val_3m, y_train, y_val = shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(SEQ_LEN,), dtype='int32')\n",
    "e = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=SEQ_LEN, trainable=False)(input_tensor)\n",
    "x = Bidirectional(CuDNNLSTM(256, return_sequences=True))(e)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(x)\n",
    "x_2 = Input(shape=(1,), dtype='float32')\n",
    "x_3 = Input(shape=(1,), dtype='float32')\n",
    "output = concatenate([x, x_2, x_3])\n",
    "output = Dense(128, activation='relu')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(7, activation='softmax')(output)\n",
    "model = Model([input_tensor, x_2, x_3], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('D:/Datasets/hackerearth/models/word2vec-lstm-6-model1.hdf5', monitor='val_acc', save_best_only=True, verbose=1, mode='max')\n",
    "model.compile(optimizer=Adam(lr=1e-3, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 60, 300)      6243000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 60, 512)      1142784     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 60, 256)      657408      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 128)          164864      bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 130)          0           bidirectional_3[0][0]            \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16768       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            903         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,225,727\n",
      "Trainable params: 1,982,727\n",
      "Non-trainable params: 6,243,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48256 samples, validate on 12065 samples\n",
      "Epoch 1/20\n",
      "48256/48256 [==============================] - 71s 1ms/step - loss: 0.5666 - acc: 0.8124 - f1: 0.8025 - val_loss: 0.4009 - val_acc: 0.8585 - val_f1: 0.8574\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85852, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model1.hdf5\n",
      "Epoch 2/20\n",
      "48256/48256 [==============================] - 64s 1ms/step - loss: 0.3951 - acc: 0.8631 - f1: 0.8611 - val_loss: 0.3772 - val_acc: 0.8681 - val_f1: 0.8675\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.85852 to 0.86813, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model1.hdf5\n",
      "Epoch 3/20\n",
      "48256/48256 [==============================] - 73s 2ms/step - loss: 0.3424 - acc: 0.8773 - f1: 0.8765 - val_loss: 0.3290 - val_acc: 0.8772 - val_f1: 0.8764\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.86813 to 0.87725, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model1.hdf5\n",
      "Epoch 4/20\n",
      "48256/48256 [==============================] - 71s 1ms/step - loss: 0.3004 - acc: 0.8901 - f1: 0.8902 - val_loss: 0.3375 - val_acc: 0.8800 - val_f1: 0.8794\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.87725 to 0.87998, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model1.hdf5\n",
      "Epoch 5/20\n",
      "48256/48256 [==============================] - 63s 1ms/step - loss: 0.2648 - acc: 0.9025 - f1: 0.9025 - val_loss: 0.3032 - val_acc: 0.8906 - val_f1: 0.8904\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.87998 to 0.89059, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model1.hdf5\n",
      "Epoch 6/20\n",
      "48256/48256 [==============================] - 61s 1ms/step - loss: 0.2256 - acc: 0.9175 - f1: 0.9173 - val_loss: 0.3054 - val_acc: 0.8884 - val_f1: 0.8884\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.89059\n",
      "Epoch 7/20\n",
      "48256/48256 [==============================] - 64s 1ms/step - loss: 0.1850 - acc: 0.9316 - f1: 0.9316 - val_loss: 0.3567 - val_acc: 0.8885 - val_f1: 0.8891\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.89059\n",
      "Epoch 8/20\n",
      "48256/48256 [==============================] - 62s 1ms/step - loss: 0.1466 - acc: 0.9466 - f1: 0.9469 - val_loss: 0.3579 - val_acc: 0.8903 - val_f1: 0.8909\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.89059\n",
      "Epoch 9/20\n",
      "48256/48256 [==============================] - 61s 1ms/step - loss: 0.1138 - acc: 0.9592 - f1: 0.9591 - val_loss: 0.3971 - val_acc: 0.8885 - val_f1: 0.8899\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.89059\n",
      "Epoch 10/20\n",
      "48256/48256 [==============================] - 61s 1ms/step - loss: 0.0918 - acc: 0.9678 - f1: 0.9678 - val_loss: 0.4603 - val_acc: 0.8945 - val_f1: 0.8951\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.89059 to 0.89449, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model1.hdf5\n",
      "Epoch 11/20\n",
      "48256/48256 [==============================] - 61s 1ms/step - loss: 0.0683 - acc: 0.9773 - f1: 0.9773 - val_loss: 0.4623 - val_acc: 0.8919 - val_f1: 0.8927\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.89449\n",
      "Epoch 12/20\n",
      "48256/48256 [==============================] - 62s 1ms/step - loss: 0.0592 - acc: 0.9810 - f1: 0.9810 - val_loss: 0.5179 - val_acc: 0.8889 - val_f1: 0.8890\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.89449\n",
      "Epoch 13/20\n",
      "48256/48256 [==============================] - 61s 1ms/step - loss: 0.0478 - acc: 0.9843 - f1: 0.9843 - val_loss: 0.5716 - val_acc: 0.8863 - val_f1: 0.8864\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.89449\n",
      "Epoch 14/20\n",
      "48256/48256 [==============================] - 61s 1ms/step - loss: 0.0402 - acc: 0.9873 - f1: 0.9872 - val_loss: 0.5944 - val_acc: 0.8947 - val_f1: 0.8948\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.89449 to 0.89465, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model1.hdf5\n",
      "Epoch 15/20\n",
      "48256/48256 [==============================] - 66s 1ms/step - loss: 0.0375 - acc: 0.9877 - f1: 0.9878 - val_loss: 0.5657 - val_acc: 0.8906 - val_f1: 0.8907\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.89465\n",
      "Epoch 16/20\n",
      "48256/48256 [==============================] - 62s 1ms/step - loss: 0.0316 - acc: 0.9900 - f1: 0.9901 - val_loss: 0.5925 - val_acc: 0.8950 - val_f1: 0.8950\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.89465 to 0.89499, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model1.hdf5\n",
      "Epoch 17/20\n",
      "48256/48256 [==============================] - 58s 1ms/step - loss: 0.0293 - acc: 0.9903 - f1: 0.9903 - val_loss: 0.6305 - val_acc: 0.8946 - val_f1: 0.8949\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.89499\n",
      "Epoch 18/20\n",
      "48256/48256 [==============================] - 57s 1ms/step - loss: 0.0291 - acc: 0.9910 - f1: 0.9909 - val_loss: 0.6634 - val_acc: 0.8918 - val_f1: 0.8926\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.89499\n",
      "Epoch 19/20\n",
      "48256/48256 [==============================] - 59s 1ms/step - loss: 0.0226 - acc: 0.9935 - f1: 0.9935 - val_loss: 0.6865 - val_acc: 0.8936 - val_f1: 0.8936\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.89499\n",
      "Epoch 20/20\n",
      "48256/48256 [==============================] - 55s 1ms/step - loss: 0.0218 - acc: 0.9932 - f1: 0.9932 - val_loss: 0.6477 - val_acc: 0.8911 - val_f1: 0.8917\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.89499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b8da777940>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train_new, x_train_24h, x_train_3m], y_train,\n",
    "           validation_data=([x_val_new, x_val_24h, x_val_3m], y_val),\n",
    "           callbacks=[checkpoint],\n",
    "           epochs=20,\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12065/12065 [==============================] - 1s 109us/step\n"
     ]
    }
   ],
   "source": [
    "score1 = model.evaluate([x_val_new, x_val_24h, x_val_3m], y_val, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6476768623631474, 0.8910899296174443, 0.8917144951512099]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new, x_val_new, x_train_24h, x_train_3m, x_val_24h, x_val_3m, y_train, y_val = shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(SEQ_LEN,), dtype='int32')\n",
    "e = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=SEQ_LEN, trainable=False)(input_tensor)\n",
    "x = Bidirectional(CuDNNLSTM(256, return_sequences=True))(e)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(x)\n",
    "x_2 = Input(shape=(1,), dtype='float32')\n",
    "x_3 = Input(shape=(1,), dtype='float32')\n",
    "output = concatenate([x, x_2, x_3])\n",
    "output = Dense(128, activation='relu')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(7, activation='softmax')(output)\n",
    "model = Model([input_tensor, x_2, x_3], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('D:/Datasets/hackerearth/models/word2vec-lstm-6-model2.hdf5', monitor='val_acc', save_best_only=True, verbose=1, mode='max')\n",
    "model.compile(optimizer=Adam(lr=1e-3, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 60, 300)      6243000     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 60, 512)      1142784     embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 60, 256)      657408      bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 128)          164864      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 130)          0           bidirectional_6[0][0]            \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          16768       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 7)            903         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,225,727\n",
      "Trainable params: 1,982,727\n",
      "Non-trainable params: 6,243,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48256 samples, validate on 12065 samples\n",
      "Epoch 1/20\n",
      "48256/48256 [==============================] - 53s 1ms/step - loss: 0.5738 - acc: 0.8088 - f1: 0.7987 - val_loss: 0.4028 - val_acc: 0.8559 - val_f1: 0.8569\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85595, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model2.hdf5\n",
      "Epoch 2/20\n",
      "48256/48256 [==============================] - 51s 1ms/step - loss: 0.3928 - acc: 0.8621 - f1: 0.8599 - val_loss: 0.3570 - val_acc: 0.8704 - val_f1: 0.8701\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.85595 to 0.87037, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model2.hdf5\n",
      "Epoch 3/20\n",
      "48256/48256 [==============================] - 51s 1ms/step - loss: 0.3445 - acc: 0.8757 - f1: 0.8747 - val_loss: 0.3467 - val_acc: 0.8705 - val_f1: 0.8703\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.87037 to 0.87053, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model2.hdf5\n",
      "Epoch 4/20\n",
      "48256/48256 [==============================] - 53s 1ms/step - loss: 0.3025 - acc: 0.8909 - f1: 0.8902 - val_loss: 0.3697 - val_acc: 0.8630 - val_f1: 0.8636\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.87053\n",
      "Epoch 5/20\n",
      "48256/48256 [==============================] - 54s 1ms/step - loss: 0.2657 - acc: 0.9013 - f1: 0.9012 - val_loss: 0.3184 - val_acc: 0.8846 - val_f1: 0.8859\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.87053 to 0.88462, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model2.hdf5\n",
      "Epoch 6/20\n",
      "48256/48256 [==============================] - 52s 1ms/step - loss: 0.2267 - acc: 0.9150 - f1: 0.9150 - val_loss: 0.3518 - val_acc: 0.8850 - val_f1: 0.8862\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.88462 to 0.88504, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model2.hdf5\n",
      "Epoch 7/20\n",
      "48256/48256 [==============================] - 58s 1ms/step - loss: 0.1900 - acc: 0.9290 - f1: 0.9292 - val_loss: 0.3398 - val_acc: 0.8899 - val_f1: 0.8913\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.88504 to 0.88993, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model2.hdf5\n",
      "Epoch 8/20\n",
      "48256/48256 [==============================] - 53s 1ms/step - loss: 0.1517 - acc: 0.9444 - f1: 0.9448 - val_loss: 0.3900 - val_acc: 0.8877 - val_f1: 0.8891\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.88993\n",
      "Epoch 9/20\n",
      "48256/48256 [==============================] - 54s 1ms/step - loss: 0.1146 - acc: 0.9588 - f1: 0.9591 - val_loss: 0.4156 - val_acc: 0.8898 - val_f1: 0.8907\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.88993\n",
      "Epoch 10/20\n",
      "48256/48256 [==============================] - 51s 1ms/step - loss: 0.0902 - acc: 0.9685 - f1: 0.9684 - val_loss: 0.4415 - val_acc: 0.8879 - val_f1: 0.8881\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.88993\n",
      "Epoch 11/20\n",
      "48256/48256 [==============================] - 51s 1ms/step - loss: 0.0702 - acc: 0.9752 - f1: 0.9753 - val_loss: 0.4811 - val_acc: 0.8888 - val_f1: 0.8892\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.88993\n",
      "Epoch 12/20\n",
      "48256/48256 [==============================] - 51s 1ms/step - loss: 0.0562 - acc: 0.9811 - f1: 0.9812 - val_loss: 0.5576 - val_acc: 0.8883 - val_f1: 0.8889\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.88993\n",
      "Epoch 13/20\n",
      "48256/48256 [==============================] - 53s 1ms/step - loss: 0.0466 - acc: 0.9846 - f1: 0.9848 - val_loss: 0.6058 - val_acc: 0.8864 - val_f1: 0.8867\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.88993\n",
      "Epoch 14/20\n",
      "48256/48256 [==============================] - 51s 1ms/step - loss: 0.0399 - acc: 0.9865 - f1: 0.9866 - val_loss: 0.5953 - val_acc: 0.8923 - val_f1: 0.8920\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.88993 to 0.89225, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model2.hdf5\n",
      "Epoch 15/20\n",
      "48256/48256 [==============================] - 51s 1ms/step - loss: 0.0361 - acc: 0.9885 - f1: 0.9884 - val_loss: 0.5902 - val_acc: 0.8884 - val_f1: 0.8892\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.89225\n",
      "Epoch 16/20\n",
      "48256/48256 [==============================] - 51s 1ms/step - loss: 0.0305 - acc: 0.9902 - f1: 0.9902 - val_loss: 0.6756 - val_acc: 0.8887 - val_f1: 0.8886\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.89225\n",
      "Epoch 17/20\n",
      "48256/48256 [==============================] - 51s 1ms/step - loss: 0.0282 - acc: 0.9910 - f1: 0.9910 - val_loss: 0.6347 - val_acc: 0.8875 - val_f1: 0.8882\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.89225\n",
      "Epoch 18/20\n",
      "48256/48256 [==============================] - 51s 1ms/step - loss: 0.0271 - acc: 0.9918 - f1: 0.9918 - val_loss: 0.6795 - val_acc: 0.8908 - val_f1: 0.8908\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.89225\n",
      "Epoch 19/20\n",
      "48256/48256 [==============================] - 51s 1ms/step - loss: 0.0257 - acc: 0.9919 - f1: 0.9920 - val_loss: 0.6378 - val_acc: 0.8858 - val_f1: 0.8861\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.89225\n",
      "Epoch 20/20\n",
      "48256/48256 [==============================] - 51s 1ms/step - loss: 0.0207 - acc: 0.9935 - f1: 0.9935 - val_loss: 0.6919 - val_acc: 0.8862 - val_f1: 0.8861\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.89225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bbb845e5c0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train_new, x_train_24h, x_train_3m], y_train,\n",
    "           validation_data=([x_val_new, x_val_24h, x_val_3m], y_val),\n",
    "           callbacks=[checkpoint],\n",
    "           epochs=20,\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12065/12065 [==============================] - 1s 108us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6919101040441369, 0.8861997514160352, 0.8861684659927453]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score2 = model.evaluate([x_val_new, x_val_24h, x_val_3m], y_val, batch_size=256, verbose=1)\n",
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new, x_val_new, x_train_24h, x_train_3m, x_val_24h, x_val_3m, y_train, y_val = shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(SEQ_LEN,), dtype='int32')\n",
    "e = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=SEQ_LEN, trainable=False)(input_tensor)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(e)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(x)\n",
    "x_2 = Input(shape=(1,), dtype='float32')\n",
    "x_3 = Input(shape=(1,), dtype='float32')\n",
    "output = concatenate([x, x_2, x_3])\n",
    "output = Dense(128, activation='relu')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(7, activation='softmax')(output)\n",
    "model = Model([input_tensor, x_2, x_3], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('D:/Datasets/hackerearth/models/word2vec-lstm-6-model3.hdf5', monitor='val_acc', save_best_only=True, verbose=1, mode='max')\n",
    "model.compile(optimizer=Adam(lr=1e-3, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 60, 300)      6243000     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 60, 256)      440320      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 128)          164864      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 130)          0           bidirectional_8[0][0]            \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          16768       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 7)            903         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,865,855\n",
      "Trainable params: 622,855\n",
      "Non-trainable params: 6,243,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48256 samples, validate on 12065 samples\n",
      "Epoch 1/20\n",
      "48256/48256 [==============================] - 37s 772us/step - loss: 0.5742 - acc: 0.8080 - f1: 0.7968 - val_loss: 0.4162 - val_acc: 0.8560 - val_f1: 0.8558\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85603, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model3.hdf5\n",
      "Epoch 2/20\n",
      "48256/48256 [==============================] - 34s 714us/step - loss: 0.3880 - acc: 0.8625 - f1: 0.8616 - val_loss: 0.3531 - val_acc: 0.8703 - val_f1: 0.8701\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.85603 to 0.87029, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model3.hdf5\n",
      "Epoch 3/20\n",
      "48256/48256 [==============================] - 34s 710us/step - loss: 0.3388 - acc: 0.8783 - f1: 0.8776 - val_loss: 0.3228 - val_acc: 0.8822 - val_f1: 0.8823\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.87029 to 0.88222, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model3.hdf5\n",
      "Epoch 4/20\n",
      "48256/48256 [==============================] - 34s 707us/step - loss: 0.2984 - acc: 0.8906 - f1: 0.8905 - val_loss: 0.3277 - val_acc: 0.8826 - val_f1: 0.8819\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.88222 to 0.88264, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model3.hdf5\n",
      "Epoch 5/20\n",
      "48256/48256 [==============================] - 34s 711us/step - loss: 0.2625 - acc: 0.9027 - f1: 0.9023 - val_loss: 0.3067 - val_acc: 0.8905 - val_f1: 0.8912\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.88264 to 0.89051, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model3.hdf5\n",
      "Epoch 6/20\n",
      "48256/48256 [==============================] - 35s 730us/step - loss: 0.2285 - acc: 0.9150 - f1: 0.9151 - val_loss: 0.3129 - val_acc: 0.8899 - val_f1: 0.8901\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.89051\n",
      "Epoch 7/20\n",
      "48256/48256 [==============================] - 35s 730us/step - loss: 0.1991 - acc: 0.9253 - f1: 0.9248 - val_loss: 0.3227 - val_acc: 0.8909 - val_f1: 0.8913\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.89051 to 0.89092, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model3.hdf5\n",
      "Epoch 8/20\n",
      "48256/48256 [==============================] - 34s 713us/step - loss: 0.1644 - acc: 0.9368 - f1: 0.9370 - val_loss: 0.3317 - val_acc: 0.8886 - val_f1: 0.8900\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.89092\n",
      "Epoch 9/20\n",
      "48256/48256 [==============================] - 34s 711us/step - loss: 0.1329 - acc: 0.9495 - f1: 0.9496 - val_loss: 0.3666 - val_acc: 0.8913 - val_f1: 0.8927\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.89092 to 0.89126, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model3.hdf5\n",
      "Epoch 10/20\n",
      "48256/48256 [==============================] - 35s 716us/step - loss: 0.1092 - acc: 0.9590 - f1: 0.9593 - val_loss: 0.4408 - val_acc: 0.8904 - val_f1: 0.8912\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.89126\n",
      "Epoch 11/20\n",
      "48256/48256 [==============================] - 34s 711us/step - loss: 0.0865 - acc: 0.9683 - f1: 0.9681 - val_loss: 0.4604 - val_acc: 0.8860 - val_f1: 0.8866\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.89126\n",
      "Epoch 12/20\n",
      "48256/48256 [==============================] - 34s 710us/step - loss: 0.0707 - acc: 0.9748 - f1: 0.9749 - val_loss: 0.5139 - val_acc: 0.8948 - val_f1: 0.8952\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.89126 to 0.89482, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model3.hdf5\n",
      "Epoch 13/20\n",
      "48256/48256 [==============================] - 34s 709us/step - loss: 0.0566 - acc: 0.9796 - f1: 0.9795 - val_loss: 0.5741 - val_acc: 0.8856 - val_f1: 0.8860\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.89482\n",
      "Epoch 14/20\n",
      "48256/48256 [==============================] - 35s 723us/step - loss: 0.0513 - acc: 0.9826 - f1: 0.9827 - val_loss: 0.5885 - val_acc: 0.8917 - val_f1: 0.8917\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.89482\n",
      "Epoch 15/20\n",
      "48256/48256 [==============================] - 34s 706us/step - loss: 0.0407 - acc: 0.9859 - f1: 0.9859 - val_loss: 0.6023 - val_acc: 0.8900 - val_f1: 0.8904\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.89482\n",
      "Epoch 16/20\n",
      "48256/48256 [==============================] - 34s 706us/step - loss: 0.0373 - acc: 0.9867 - f1: 0.9868 - val_loss: 0.6356 - val_acc: 0.8933 - val_f1: 0.8940\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.89482\n",
      "Epoch 17/20\n",
      "48256/48256 [==============================] - 34s 708us/step - loss: 0.0327 - acc: 0.9891 - f1: 0.9891 - val_loss: 0.6104 - val_acc: 0.8946 - val_f1: 0.8947\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.89482\n",
      "Epoch 18/20\n",
      "48256/48256 [==============================] - 34s 706us/step - loss: 0.0274 - acc: 0.9909 - f1: 0.9908 - val_loss: 0.6655 - val_acc: 0.8927 - val_f1: 0.8930\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.89482\n",
      "Epoch 19/20\n",
      "48256/48256 [==============================] - 35s 734us/step - loss: 0.0281 - acc: 0.9905 - f1: 0.9905 - val_loss: 0.6746 - val_acc: 0.8931 - val_f1: 0.8938\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.89482\n",
      "Epoch 20/20\n",
      "48256/48256 [==============================] - 35s 718us/step - loss: 0.0241 - acc: 0.9920 - f1: 0.9920 - val_loss: 0.7173 - val_acc: 0.8942 - val_f1: 0.8942\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.89482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b8da777a20>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train_new, x_train_24h, x_train_3m], y_train,\n",
    "           validation_data=([x_val_new, x_val_24h, x_val_3m], y_val),\n",
    "           callbacks=[checkpoint],\n",
    "           epochs=20,\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12065/12065 [==============================] - 1s 54us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7172850785885814, 0.8942395358376122, 0.8941674241424842]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score3 = model.evaluate([x_val_new, x_val_24h, x_val_3m], y_val, batch_size=256, verbose=1)\n",
    "score3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new, x_val_new, x_train_24h, x_train_3m, x_val_24h, x_val_3m, y_train, y_val = shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(SEQ_LEN,), dtype='int32')\n",
    "e = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=SEQ_LEN, trainable=False)(input_tensor)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(e)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(x)\n",
    "x_2 = Input(shape=(1,), dtype='float32')\n",
    "x_3 = Input(shape=(1,), dtype='float32')\n",
    "output = concatenate([x, x_2, x_3])\n",
    "output = Dense(128, activation='relu')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(7, activation='softmax')(output)\n",
    "model = Model([input_tensor, x_2, x_3], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('D:/Datasets/hackerearth/models/word2vec-lstm-6-model4.hdf5', monitor='val_acc', save_best_only=True, verbose=1, mode='max')\n",
    "model.compile(optimizer=Adam(lr=1e-3, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 60, 300)      6243000     input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 60, 256)      440320      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 128)          164864      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 130)          0           bidirectional_10[0][0]           \n",
      "                                                                 input_11[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          16768       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 7)            903         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,865,855\n",
      "Trainable params: 622,855\n",
      "Non-trainable params: 6,243,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48256 samples, validate on 12065 samples\n",
      "Epoch 1/20\n",
      "48256/48256 [==============================] - 37s 771us/step - loss: 0.5507 - acc: 0.8134 - f1: 0.8034 - val_loss: 0.3940 - val_acc: 0.8604 - val_f1: 0.8577\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.86042, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model4.hdf5\n",
      "Epoch 2/20\n",
      "48256/48256 [==============================] - 35s 723us/step - loss: 0.3833 - acc: 0.8631 - f1: 0.8616 - val_loss: 0.3522 - val_acc: 0.8685 - val_f1: 0.8690\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.86042 to 0.86855, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model4.hdf5\n",
      "Epoch 3/20\n",
      "48256/48256 [==============================] - 35s 726us/step - loss: 0.3348 - acc: 0.8781 - f1: 0.8777 - val_loss: 0.3341 - val_acc: 0.8758 - val_f1: 0.8750\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.86855 to 0.87576, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model4.hdf5\n",
      "Epoch 4/20\n",
      "48256/48256 [==============================] - 35s 726us/step - loss: 0.2981 - acc: 0.8905 - f1: 0.8891 - val_loss: 0.3529 - val_acc: 0.8772 - val_f1: 0.8773\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.87576 to 0.87717, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model4.hdf5\n",
      "Epoch 5/20\n",
      "48256/48256 [==============================] - 35s 734us/step - loss: 0.2607 - acc: 0.9029 - f1: 0.9027 - val_loss: 0.3203 - val_acc: 0.8869 - val_f1: 0.8856\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.87717 to 0.88695, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model4.hdf5\n",
      "Epoch 6/20\n",
      "48256/48256 [==============================] - 36s 745us/step - loss: 0.2276 - acc: 0.9139 - f1: 0.9139 - val_loss: 0.3432 - val_acc: 0.8830 - val_f1: 0.8829\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.88695\n",
      "Epoch 7/20\n",
      "48256/48256 [==============================] - 35s 724us/step - loss: 0.1983 - acc: 0.9252 - f1: 0.9252 - val_loss: 0.3605 - val_acc: 0.8838 - val_f1: 0.8841\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.88695\n",
      "Epoch 8/20\n",
      "48256/48256 [==============================] - 35s 715us/step - loss: 0.1613 - acc: 0.9403 - f1: 0.9403 - val_loss: 0.3707 - val_acc: 0.8918 - val_f1: 0.8926\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.88695 to 0.89175, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model4.hdf5\n",
      "Epoch 9/20\n",
      "48256/48256 [==============================] - 34s 714us/step - loss: 0.1307 - acc: 0.9509 - f1: 0.9517 - val_loss: 0.4089 - val_acc: 0.8872 - val_f1: 0.8876\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.89175\n",
      "Epoch 10/20\n",
      "48256/48256 [==============================] - 35s 718us/step - loss: 0.1088 - acc: 0.9595 - f1: 0.9598 - val_loss: 0.4284 - val_acc: 0.8904 - val_f1: 0.8902\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.89175\n",
      "Epoch 11/20\n",
      "48256/48256 [==============================] - 35s 717us/step - loss: 0.0876 - acc: 0.9692 - f1: 0.9694 - val_loss: 0.4484 - val_acc: 0.8899 - val_f1: 0.8902\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.89175\n",
      "Epoch 12/20\n",
      "48256/48256 [==============================] - 35s 715us/step - loss: 0.0677 - acc: 0.9759 - f1: 0.9760 - val_loss: 0.4899 - val_acc: 0.8910 - val_f1: 0.8916\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.89175\n",
      "Epoch 13/20\n",
      "48256/48256 [==============================] - 34s 715us/step - loss: 0.0596 - acc: 0.9802 - f1: 0.9803 - val_loss: 0.5285 - val_acc: 0.8884 - val_f1: 0.8885\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.89175\n",
      "Epoch 14/20\n",
      "48256/48256 [==============================] - 34s 714us/step - loss: 0.0480 - acc: 0.9831 - f1: 0.9832 - val_loss: 0.6068 - val_acc: 0.8903 - val_f1: 0.8907\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.89175\n",
      "Epoch 15/20\n",
      "48256/48256 [==============================] - 35s 729us/step - loss: 0.0401 - acc: 0.9859 - f1: 0.9859 - val_loss: 0.6142 - val_acc: 0.8881 - val_f1: 0.8883\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.89175\n",
      "Epoch 16/20\n",
      "48256/48256 [==============================] - 35s 725us/step - loss: 0.0371 - acc: 0.9878 - f1: 0.9878 - val_loss: 0.5922 - val_acc: 0.8891 - val_f1: 0.8898\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.89175\n",
      "Epoch 17/20\n",
      "48256/48256 [==============================] - 36s 739us/step - loss: 0.0342 - acc: 0.9887 - f1: 0.9888 - val_loss: 0.5951 - val_acc: 0.8914 - val_f1: 0.8916\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.89175\n",
      "Epoch 18/20\n",
      "48256/48256 [==============================] - 34s 709us/step - loss: 0.0285 - acc: 0.9907 - f1: 0.9907 - val_loss: 0.6266 - val_acc: 0.8893 - val_f1: 0.8898\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.89175\n",
      "Epoch 19/20\n",
      "48256/48256 [==============================] - 34s 704us/step - loss: 0.0295 - acc: 0.9909 - f1: 0.9911 - val_loss: 0.5919 - val_acc: 0.8913 - val_f1: 0.8917\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.89175\n",
      "Epoch 20/20\n",
      "48256/48256 [==============================] - 35s 726us/step - loss: 0.0244 - acc: 0.9916 - f1: 0.9916 - val_loss: 0.6838 - val_acc: 0.8910 - val_f1: 0.8914\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.89175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bbbea37748>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train_new, x_train_24h, x_train_3m], y_train,\n",
    "           validation_data=([x_val_new, x_val_24h, x_val_3m], y_val),\n",
    "           callbacks=[checkpoint],\n",
    "           epochs=20,\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12065/12065 [==============================] - 1s 56us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6838089465600942, 0.8910070452460895, 0.8914537867618407]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score4 = model.evaluate([x_val_new, x_val_24h, x_val_3m], y_val, batch_size=256, verbose=1)\n",
    "score4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new, x_val_new, x_train_24h, x_train_3m, x_val_24h, x_val_3m, y_train, y_val = shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(SEQ_LEN,), dtype='int32')\n",
    "e = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=SEQ_LEN, trainable=False)(input_tensor)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(e)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(x)\n",
    "x_2 = Input(shape=(1,), dtype='float32')\n",
    "x_3 = Input(shape=(1,), dtype='float32')\n",
    "output = concatenate([x, x_2, x_3])\n",
    "output = Dense(256, activation='relu')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(64, activation='relu')(output)\n",
    "output = Dropout(0.2)(output)\n",
    "output = Dense(7, activation='softmax')(output)\n",
    "model = Model([input_tensor, x_2, x_3], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('D:/Datasets/hackerearth/models/word2vec-lstm-6-model5.hdf5', monitor='val_acc', save_best_only=True, verbose=1, mode='max')\n",
    "model.compile(optimizer=Adam(lr=1e-3, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 60, 300)      6243000     input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 60, 256)      440320      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 128)          164864      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 130)          0           bidirectional_12[0][0]           \n",
      "                                                                 input_14[0][0]                   \n",
      "                                                                 input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          33536       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 256)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 64)           16448       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64)           0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 7)            455         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,898,623\n",
      "Trainable params: 655,623\n",
      "Non-trainable params: 6,243,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48256 samples, validate on 12065 samples\n",
      "Epoch 1/20\n",
      "48256/48256 [==============================] - 45s 932us/step - loss: 0.5800 - acc: 0.8049 - f1: 0.7964 - val_loss: 0.4051 - val_acc: 0.8576 - val_f1: 0.8567\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85760, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model5.hdf5\n",
      "Epoch 2/20\n",
      "48256/48256 [==============================] - 39s 817us/step - loss: 0.3974 - acc: 0.8609 - f1: 0.8592 - val_loss: 0.3692 - val_acc: 0.8645 - val_f1: 0.8656\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.85760 to 0.86448, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model5.hdf5\n",
      "Epoch 3/20\n",
      "48256/48256 [==============================] - 44s 917us/step - loss: 0.3451 - acc: 0.8781 - f1: 0.8764 - val_loss: 0.3375 - val_acc: 0.8763 - val_f1: 0.8760\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.86448 to 0.87634, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model5.hdf5\n",
      "Epoch 4/20\n",
      "48256/48256 [==============================] - 42s 877us/step - loss: 0.3074 - acc: 0.8886 - f1: 0.8884 - val_loss: 0.3302 - val_acc: 0.8813 - val_f1: 0.8813\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.87634 to 0.88131, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model5.hdf5\n",
      "Epoch 5/20\n",
      "48256/48256 [==============================] - 42s 862us/step - loss: 0.2716 - acc: 0.8993 - f1: 0.8997 - val_loss: 0.3188 - val_acc: 0.8835 - val_f1: 0.8852\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.88131 to 0.88346, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model5.hdf5\n",
      "Epoch 6/20\n",
      "48256/48256 [==============================] - 49s 1ms/step - loss: 0.2400 - acc: 0.9112 - f1: 0.9103 - val_loss: 0.3140 - val_acc: 0.8859 - val_f1: 0.8859\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.88346 to 0.88587, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model5.hdf5\n",
      "Epoch 7/20\n",
      "48256/48256 [==============================] - 43s 895us/step - loss: 0.2050 - acc: 0.9242 - f1: 0.9238 - val_loss: 0.3520 - val_acc: 0.8860 - val_f1: 0.8864\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.88587 to 0.88595, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model5.hdf5\n",
      "Epoch 8/20\n",
      "48256/48256 [==============================] - 41s 854us/step - loss: 0.1783 - acc: 0.9343 - f1: 0.9343 - val_loss: 0.3636 - val_acc: 0.8844 - val_f1: 0.8850\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.88595\n",
      "Epoch 9/20\n",
      "48256/48256 [==============================] - 45s 930us/step - loss: 0.1461 - acc: 0.9455 - f1: 0.9452 - val_loss: 0.3784 - val_acc: 0.8879 - val_f1: 0.8874\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.88595 to 0.88794, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model5.hdf5\n",
      "Epoch 10/20\n",
      "48256/48256 [==============================] - 42s 861us/step - loss: 0.1205 - acc: 0.9561 - f1: 0.9562 - val_loss: 0.4545 - val_acc: 0.8877 - val_f1: 0.8873\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.88794\n",
      "Epoch 11/20\n",
      "48256/48256 [==============================] - 38s 786us/step - loss: 0.0974 - acc: 0.9641 - f1: 0.9643 - val_loss: 0.4610 - val_acc: 0.8847 - val_f1: 0.8854\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.88794\n",
      "Epoch 12/20\n",
      "48256/48256 [==============================] - 38s 782us/step - loss: 0.0795 - acc: 0.9715 - f1: 0.9717 - val_loss: 0.5314 - val_acc: 0.8858 - val_f1: 0.8864\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.88794\n",
      "Epoch 13/20\n",
      "48256/48256 [==============================] - 41s 843us/step - loss: 0.0664 - acc: 0.9766 - f1: 0.9766 - val_loss: 0.5043 - val_acc: 0.8796 - val_f1: 0.8805\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.88794\n",
      "Epoch 14/20\n",
      "48256/48256 [==============================] - 40s 832us/step - loss: 0.0584 - acc: 0.9796 - f1: 0.9797 - val_loss: 0.5400 - val_acc: 0.8889 - val_f1: 0.8892\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.88794 to 0.88885, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model5.hdf5\n",
      "Epoch 15/20\n",
      "48256/48256 [==============================] - 39s 798us/step - loss: 0.0493 - acc: 0.9834 - f1: 0.9836 - val_loss: 0.6430 - val_acc: 0.8893 - val_f1: 0.8893\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.88885 to 0.88927, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model5.hdf5\n",
      "Epoch 16/20\n",
      "48256/48256 [==============================] - 37s 766us/step - loss: 0.0432 - acc: 0.9861 - f1: 0.9862 - val_loss: 0.6383 - val_acc: 0.8920 - val_f1: 0.8922\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.88927 to 0.89200, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model5.hdf5\n",
      "Epoch 17/20\n",
      "48256/48256 [==============================] - 40s 837us/step - loss: 0.0405 - acc: 0.9870 - f1: 0.9870 - val_loss: 0.6950 - val_acc: 0.8896 - val_f1: 0.8901\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.89200\n",
      "Epoch 18/20\n",
      "48256/48256 [==============================] - 36s 754us/step - loss: 0.0358 - acc: 0.9879 - f1: 0.9880 - val_loss: 0.6175 - val_acc: 0.8908 - val_f1: 0.8911\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.89200\n",
      "Epoch 19/20\n",
      "48256/48256 [==============================] - 35s 730us/step - loss: 0.0334 - acc: 0.9888 - f1: 0.9888 - val_loss: 0.6307 - val_acc: 0.8888 - val_f1: 0.8891\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.89200\n",
      "Epoch 20/20\n",
      "48256/48256 [==============================] - 35s 728us/step - loss: 0.0289 - acc: 0.9906 - f1: 0.9905 - val_loss: 0.7376 - val_acc: 0.8871 - val_f1: 0.8871\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.89200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bbbdf27c88>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train_new, x_train_24h, x_train_3m], y_train,\n",
    "           validation_data=([x_val_new, x_val_24h, x_val_3m], y_val),\n",
    "           callbacks=[checkpoint],\n",
    "           epochs=20,\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12065/12065 [==============================] - 1s 54us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7376303530312454, 0.8871114795602213, 0.8870814942641718]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score5 = model.evaluate([x_val_new, x_val_24h, x_val_3m], y_val, batch_size=256, verbose=1)\n",
    "score5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new, x_val_new, x_train_24h, x_train_3m, x_val_24h, x_val_3m, y_train, y_val = shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(SEQ_LEN,), dtype='int32')\n",
    "e = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=SEQ_LEN, trainable=False)(input_tensor)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(e)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(x)\n",
    "x_2 = Input(shape=(1,), dtype='float32')\n",
    "x_3 = Input(shape=(1,), dtype='float32')\n",
    "output = concatenate([x, x_2, x_3])\n",
    "output = Dense(256, activation='relu')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(64, activation='relu')(output)\n",
    "output = Dropout(0.2)(output)\n",
    "output = Dense(7, activation='softmax')(output)\n",
    "model = Model([input_tensor, x_2, x_3], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 60, 300)      6243000     input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 60, 256)      440320      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 128)          164864      bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 130)          0           bidirectional_14[0][0]           \n",
      "                                                                 input_17[0][0]                   \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 256)          33536       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256)          0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 64)           16448       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64)           0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 7)            455         dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,898,623\n",
      "Trainable params: 655,623\n",
      "Non-trainable params: 6,243,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('D:/Datasets/hackerearth/models/word2vec-lstm-6-model6.hdf5', monitor='val_acc', save_best_only=True, verbose=1, mode='max')\n",
    "model.compile(optimizer=Adam(lr=1e-3, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy', f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48256 samples, validate on 12065 samples\n",
      "Epoch 1/20\n",
      "48256/48256 [==============================] - 38s 786us/step - loss: 0.5825 - acc: 0.8043 - f1: 0.7921 - val_loss: 0.3957 - val_acc: 0.8605 - val_f1: 0.8551\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.86051, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model6.hdf5\n",
      "Epoch 2/20\n",
      "48256/48256 [==============================] - 36s 741us/step - loss: 0.3995 - acc: 0.8601 - f1: 0.8582 - val_loss: 0.3383 - val_acc: 0.8782 - val_f1: 0.8763\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.86051 to 0.87816, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model6.hdf5\n",
      "Epoch 3/20\n",
      "48256/48256 [==============================] - 36s 739us/step - loss: 0.3488 - acc: 0.8751 - f1: 0.8737 - val_loss: 0.3284 - val_acc: 0.8797 - val_f1: 0.8754\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.87816 to 0.87965, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model6.hdf5\n",
      "Epoch 4/20\n",
      "48256/48256 [==============================] - 36s 737us/step - loss: 0.3080 - acc: 0.8883 - f1: 0.8873 - val_loss: 0.3093 - val_acc: 0.8870 - val_f1: 0.8884\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.87965 to 0.88703, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model6.hdf5\n",
      "Epoch 5/20\n",
      "48256/48256 [==============================] - 36s 738us/step - loss: 0.2726 - acc: 0.8994 - f1: 0.8989 - val_loss: 0.3083 - val_acc: 0.8902 - val_f1: 0.8907\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.88703 to 0.89018, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model6.hdf5\n",
      "Epoch 6/20\n",
      "48256/48256 [==============================] - 36s 740us/step - loss: 0.2396 - acc: 0.9108 - f1: 0.9108 - val_loss: 0.3189 - val_acc: 0.8864 - val_f1: 0.8867\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.89018\n",
      "Epoch 7/20\n",
      "48256/48256 [==============================] - 36s 738us/step - loss: 0.2067 - acc: 0.9236 - f1: 0.9240 - val_loss: 0.3167 - val_acc: 0.8928 - val_f1: 0.8931\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.89018 to 0.89283, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model6.hdf5\n",
      "Epoch 8/20\n",
      "48256/48256 [==============================] - 36s 738us/step - loss: 0.1751 - acc: 0.9337 - f1: 0.9343 - val_loss: 0.3282 - val_acc: 0.8948 - val_f1: 0.8947\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.89283 to 0.89482, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model6.hdf5\n",
      "Epoch 9/20\n",
      "48256/48256 [==============================] - 36s 739us/step - loss: 0.1446 - acc: 0.9471 - f1: 0.9472 - val_loss: 0.3727 - val_acc: 0.8869 - val_f1: 0.8886\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.89482\n",
      "Epoch 10/20\n",
      "48256/48256 [==============================] - 36s 737us/step - loss: 0.1188 - acc: 0.9562 - f1: 0.9565 - val_loss: 0.4038 - val_acc: 0.8892 - val_f1: 0.8903\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.89482\n",
      "Epoch 11/20\n",
      "48256/48256 [==============================] - 36s 738us/step - loss: 0.0980 - acc: 0.9654 - f1: 0.9654 - val_loss: 0.4526 - val_acc: 0.8929 - val_f1: 0.8933\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.89482\n",
      "Epoch 12/20\n",
      "48256/48256 [==============================] - 36s 739us/step - loss: 0.0830 - acc: 0.9716 - f1: 0.9718 - val_loss: 0.4526 - val_acc: 0.8905 - val_f1: 0.8907\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.89482\n",
      "Epoch 13/20\n",
      "48256/48256 [==============================] - 36s 736us/step - loss: 0.0685 - acc: 0.9770 - f1: 0.9771 - val_loss: 0.5116 - val_acc: 0.8933 - val_f1: 0.8938\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.89482\n",
      "Epoch 14/20\n",
      "48256/48256 [==============================] - 35s 735us/step - loss: 0.0581 - acc: 0.9805 - f1: 0.9806 - val_loss: 0.5005 - val_acc: 0.8850 - val_f1: 0.8855\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.89482\n",
      "Epoch 15/20\n",
      "48256/48256 [==============================] - 36s 736us/step - loss: 0.0498 - acc: 0.9832 - f1: 0.9833 - val_loss: 0.5404 - val_acc: 0.8934 - val_f1: 0.8935\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.89482\n",
      "Epoch 16/20\n",
      "48256/48256 [==============================] - 36s 736us/step - loss: 0.0419 - acc: 0.9861 - f1: 0.9861 - val_loss: 0.6048 - val_acc: 0.8918 - val_f1: 0.8920\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.89482\n",
      "Epoch 17/20\n",
      "48256/48256 [==============================] - 36s 736us/step - loss: 0.0389 - acc: 0.9871 - f1: 0.9871 - val_loss: 0.6084 - val_acc: 0.8906 - val_f1: 0.8913\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.89482\n",
      "Epoch 18/20\n",
      "48256/48256 [==============================] - 36s 736us/step - loss: 0.0336 - acc: 0.9893 - f1: 0.9893 - val_loss: 0.6891 - val_acc: 0.8918 - val_f1: 0.8918\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.89482\n",
      "Epoch 19/20\n",
      "48256/48256 [==============================] - 36s 736us/step - loss: 0.0339 - acc: 0.9889 - f1: 0.9889 - val_loss: 0.6901 - val_acc: 0.8901 - val_f1: 0.8908\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.89482\n",
      "Epoch 20/20\n",
      "48256/48256 [==============================] - 35s 734us/step - loss: 0.0300 - acc: 0.9902 - f1: 0.9902 - val_loss: 0.7967 - val_acc: 0.8779 - val_f1: 0.8778\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.89482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bbc3597550>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train_new, x_train_24h, x_train_3m], y_train,\n",
    "           validation_data=([x_val_new, x_val_24h, x_val_3m], y_val),\n",
    "           callbacks=[checkpoint],\n",
    "           epochs=20,\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12065/12065 [==============================] - 1s 54us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7967415176483293, 0.8779113137025434, 0.8778340134342034]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score6 = model.evaluate([x_val_new, x_val_24h, x_val_3m], y_val, batch_size=256, verbose=1)\n",
    "score6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new, x_val_new, x_train_24h, x_train_3m, x_val_24h, x_val_3m, y_train, y_val = shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(SEQ_LEN,), dtype='int32')\n",
    "e = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=SEQ_LEN, trainable=False)(input_tensor)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(e)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(x)\n",
    "x_2 = Input(shape=(1,), dtype='float32')\n",
    "x_3 = Input(shape=(1,), dtype='float32')\n",
    "output = concatenate([x, x_2, x_3])\n",
    "output = Dense(256, activation='relu')(output)\n",
    "output = Dropout(0.6)(output)\n",
    "output = Dense(128, activation='relu')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(64, activation='relu')(output)\n",
    "output = Dropout(0.4)(output)\n",
    "output = Dense(7, activation='softmax')(output)\n",
    "model = Model([input_tensor, x_2, x_3], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 60, 300)      6243000     input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional (None, 60, 256)      440320      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, 128)          164864      bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 130)          0           bidirectional_16[0][0]           \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 256)          33536       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256)          0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 128)          32896       dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128)          0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 64)           8256        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 64)           0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 7)            455         dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,923,327\n",
      "Trainable params: 680,327\n",
      "Non-trainable params: 6,243,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('D:/Datasets/hackerearth/models/word2vec-lstm-6-model7.hdf5', monitor='val_acc', save_best_only=True, verbose=1, mode='max')\n",
    "model.compile(optimizer=Adam(lr=1e-3, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy', f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48256 samples, validate on 12065 samples\n",
      "Epoch 1/20\n",
      "48256/48256 [==============================] - 39s 799us/step - loss: 0.7310 - acc: 0.7562 - f1: 0.7408 - val_loss: 0.4638 - val_acc: 0.8427 - val_f1: 0.8396\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.84269, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model7.hdf5\n",
      "Epoch 2/20\n",
      "48256/48256 [==============================] - 37s 762us/step - loss: 0.4810 - acc: 0.8445 - f1: 0.8371 - val_loss: 0.4062 - val_acc: 0.8566 - val_f1: 0.8561\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.84269 to 0.85661, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model7.hdf5\n",
      "Epoch 3/20\n",
      "48256/48256 [==============================] - 37s 762us/step - loss: 0.4129 - acc: 0.8639 - f1: 0.8600 - val_loss: 0.3548 - val_acc: 0.8767 - val_f1: 0.8748\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.85661 to 0.87667, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model7.hdf5\n",
      "Epoch 4/20\n",
      "48256/48256 [==============================] - 37s 760us/step - loss: 0.3654 - acc: 0.8771 - f1: 0.8740 - val_loss: 0.3685 - val_acc: 0.8702 - val_f1: 0.8672\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.87667\n",
      "Epoch 5/20\n",
      "48256/48256 [==============================] - 37s 762us/step - loss: 0.3338 - acc: 0.8884 - f1: 0.8861 - val_loss: 0.3391 - val_acc: 0.8841 - val_f1: 0.8824\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.87667 to 0.88413, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model7.hdf5\n",
      "Epoch 6/20\n",
      "48256/48256 [==============================] - 37s 761us/step - loss: 0.3034 - acc: 0.8973 - f1: 0.8954 - val_loss: 0.3516 - val_acc: 0.8805 - val_f1: 0.8801\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.88413\n",
      "Epoch 7/20\n",
      "48256/48256 [==============================] - 37s 761us/step - loss: 0.2656 - acc: 0.9073 - f1: 0.9070 - val_loss: 0.3750 - val_acc: 0.8800 - val_f1: 0.8796\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.88413\n",
      "Epoch 8/20\n",
      "48256/48256 [==============================] - 37s 763us/step - loss: 0.2431 - acc: 0.9176 - f1: 0.9171 - val_loss: 0.3457 - val_acc: 0.8920 - val_f1: 0.8941\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.88413 to 0.89200, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model7.hdf5\n",
      "Epoch 9/20\n",
      "48256/48256 [==============================] - 37s 760us/step - loss: 0.2098 - acc: 0.9279 - f1: 0.9278 - val_loss: 0.3423 - val_acc: 0.8913 - val_f1: 0.8904\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.89200\n",
      "Epoch 10/20\n",
      "48256/48256 [==============================] - 37s 760us/step - loss: 0.1865 - acc: 0.9364 - f1: 0.9366 - val_loss: 0.4066 - val_acc: 0.8917 - val_f1: 0.8925\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.89200\n",
      "Epoch 11/20\n",
      "48256/48256 [==============================] - 37s 763us/step - loss: 0.1618 - acc: 0.9453 - f1: 0.9452 - val_loss: 0.4313 - val_acc: 0.8913 - val_f1: 0.8926\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.89200\n",
      "Epoch 12/20\n",
      "48256/48256 [==============================] - 37s 761us/step - loss: 0.1430 - acc: 0.9520 - f1: 0.9521 - val_loss: 0.4745 - val_acc: 0.8913 - val_f1: 0.8923\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.89200\n",
      "Epoch 13/20\n",
      "48256/48256 [==============================] - 37s 758us/step - loss: 0.1226 - acc: 0.9589 - f1: 0.9588 - val_loss: 0.5341 - val_acc: 0.8880 - val_f1: 0.8888\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.89200\n",
      "Epoch 14/20\n",
      "48256/48256 [==============================] - 37s 763us/step - loss: 0.1062 - acc: 0.9661 - f1: 0.9658 - val_loss: 0.5095 - val_acc: 0.8967 - val_f1: 0.8976\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.89200 to 0.89673, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model7.hdf5\n",
      "Epoch 15/20\n",
      "48256/48256 [==============================] - 37s 766us/step - loss: 0.0957 - acc: 0.9698 - f1: 0.9699 - val_loss: 0.5818 - val_acc: 0.8964 - val_f1: 0.8975\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.89673\n",
      "Epoch 16/20\n",
      "48256/48256 [==============================] - 37s 763us/step - loss: 0.0818 - acc: 0.9755 - f1: 0.9756 - val_loss: 0.5867 - val_acc: 0.8915 - val_f1: 0.8919\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.89673\n",
      "Epoch 17/20\n",
      "48256/48256 [==============================] - 37s 761us/step - loss: 0.0746 - acc: 0.9783 - f1: 0.9783 - val_loss: 0.6491 - val_acc: 0.8943 - val_f1: 0.8954\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.89673\n",
      "Epoch 18/20\n",
      "48256/48256 [==============================] - 38s 793us/step - loss: 0.0658 - acc: 0.9806 - f1: 0.9805 - val_loss: 0.6212 - val_acc: 0.8954 - val_f1: 0.8965\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.89673\n",
      "Epoch 19/20\n",
      "48256/48256 [==============================] - 37s 762us/step - loss: 0.0624 - acc: 0.9821 - f1: 0.9822 - val_loss: 0.6756 - val_acc: 0.8952 - val_f1: 0.8959\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.89673\n",
      "Epoch 20/20\n",
      "48256/48256 [==============================] - 37s 763us/step - loss: 0.0535 - acc: 0.9846 - f1: 0.9850 - val_loss: 0.6368 - val_acc: 0.8920 - val_f1: 0.8929\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.89673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bbc3597390>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train_new, x_train_24h, x_train_3m], y_train,\n",
    "           validation_data=([x_val_new, x_val_24h, x_val_3m], y_val),\n",
    "           callbacks=[checkpoint],\n",
    "           epochs=20,\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12065/12065 [==============================] - 1s 55us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6367931431005764, 0.89200165775669, 0.8928826240647447]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score7 = model.evaluate([x_val_new, x_val_24h, x_val_3m], y_val, batch_size=256, verbose=1)\n",
    "score7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new, x_val_new, x_train_24h, x_train_3m, x_val_24h, x_val_3m, y_train, y_val = shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(SEQ_LEN,), dtype='int32')\n",
    "e = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=SEQ_LEN, trainable=False)(input_tensor)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(e)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(x)\n",
    "x_2 = Input(shape=(1,), dtype='float32')\n",
    "x_3 = Input(shape=(1,), dtype='float32')\n",
    "output = concatenate([x, x_2, x_3])\n",
    "output = Dense(256, activation='relu')(output)\n",
    "output = Dropout(0.6)(output)\n",
    "output = Dense(128, activation='relu')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(64, activation='relu')(output)\n",
    "output = Dropout(0.4)(output)\n",
    "output = Dense(7, activation='softmax')(output)\n",
    "model = Model([input_tensor, x_2, x_3], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 60, 300)      6243000     input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, 60, 256)      440320      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional (None, 128)          164864      bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 130)          0           bidirectional_18[0][0]           \n",
      "                                                                 input_23[0][0]                   \n",
      "                                                                 input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 256)          33536       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 256)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 128)          32896       dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 128)          0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 64)           8256        dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 64)           0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 7)            455         dropout_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,923,327\n",
      "Trainable params: 680,327\n",
      "Non-trainable params: 6,243,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('D:/Datasets/hackerearth/models/word2vec-lstm-6-model8.hdf5', monitor='val_acc', save_best_only=True, verbose=1, mode='max')\n",
    "model.compile(optimizer=Adam(lr=1e-3, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy', f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48256 samples, validate on 12065 samples\n",
      "Epoch 1/20\n",
      "48256/48256 [==============================] - 39s 805us/step - loss: 0.7169 - acc: 0.7620 - f1: 0.7410 - val_loss: 0.4857 - val_acc: 0.8424 - val_f1: 0.8331\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.84244, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model8.hdf5\n",
      "Epoch 2/20\n",
      "48256/48256 [==============================] - 37s 763us/step - loss: 0.4706 - acc: 0.8466 - f1: 0.8418 - val_loss: 0.4099 - val_acc: 0.8616 - val_f1: 0.8614\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.84244 to 0.86158, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model8.hdf5\n",
      "Epoch 3/20\n",
      "48256/48256 [==============================] - 37s 762us/step - loss: 0.4089 - acc: 0.8637 - f1: 0.8594 - val_loss: 0.3881 - val_acc: 0.8687 - val_f1: 0.8637\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.86158 to 0.86871, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model8.hdf5\n",
      "Epoch 4/20\n",
      "48256/48256 [==============================] - 37s 762us/step - loss: 0.3653 - acc: 0.8773 - f1: 0.8740 - val_loss: 0.3602 - val_acc: 0.8753 - val_f1: 0.8761\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.86871 to 0.87526, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model8.hdf5\n",
      "Epoch 5/20\n",
      "48256/48256 [==============================] - 37s 762us/step - loss: 0.3319 - acc: 0.8884 - f1: 0.8870 - val_loss: 0.3453 - val_acc: 0.8767 - val_f1: 0.8777\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.87526 to 0.87667, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model8.hdf5\n",
      "Epoch 6/20\n",
      "48256/48256 [==============================] - 37s 763us/step - loss: 0.3029 - acc: 0.8974 - f1: 0.8962 - val_loss: 0.3831 - val_acc: 0.8789 - val_f1: 0.8784\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.87667 to 0.87891, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model8.hdf5\n",
      "Epoch 7/20\n",
      "48256/48256 [==============================] - 37s 762us/step - loss: 0.2672 - acc: 0.9087 - f1: 0.9073 - val_loss: 0.3756 - val_acc: 0.8809 - val_f1: 0.8811\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.87891 to 0.88090, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model8.hdf5\n",
      "Epoch 8/20\n",
      "48256/48256 [==============================] - 37s 760us/step - loss: 0.2380 - acc: 0.9168 - f1: 0.9164 - val_loss: 0.4138 - val_acc: 0.8809 - val_f1: 0.8807\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.88090\n",
      "Epoch 9/20\n",
      "48256/48256 [==============================] - 36s 755us/step - loss: 0.2080 - acc: 0.9271 - f1: 0.9261 - val_loss: 0.3946 - val_acc: 0.8853 - val_f1: 0.8861\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.88090 to 0.88529, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model8.hdf5\n",
      "Epoch 10/20\n",
      "48256/48256 [==============================] - 37s 765us/step - loss: 0.1868 - acc: 0.9355 - f1: 0.9347 - val_loss: 0.4271 - val_acc: 0.8877 - val_f1: 0.8892\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.88529 to 0.88769, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model8.hdf5\n",
      "Epoch 11/20\n",
      "48256/48256 [==============================] - 37s 765us/step - loss: 0.1601 - acc: 0.9463 - f1: 0.9465 - val_loss: 0.4572 - val_acc: 0.8851 - val_f1: 0.8860\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.88769\n",
      "Epoch 12/20\n",
      "48256/48256 [==============================] - 37s 762us/step - loss: 0.1426 - acc: 0.9536 - f1: 0.9532 - val_loss: 0.4736 - val_acc: 0.8864 - val_f1: 0.8866\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.88769\n",
      "Epoch 13/20\n",
      "48256/48256 [==============================] - 37s 765us/step - loss: 0.1157 - acc: 0.9623 - f1: 0.9622 - val_loss: 0.5481 - val_acc: 0.8919 - val_f1: 0.8925\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.88769 to 0.89192, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model8.hdf5\n",
      "Epoch 14/20\n",
      "48256/48256 [==============================] - 37s 765us/step - loss: 0.1069 - acc: 0.9657 - f1: 0.9656 - val_loss: 0.5701 - val_acc: 0.8880 - val_f1: 0.8887\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.89192\n",
      "Epoch 15/20\n",
      "48256/48256 [==============================] - 37s 763us/step - loss: 0.0927 - acc: 0.9703 - f1: 0.9704 - val_loss: 0.6226 - val_acc: 0.8787 - val_f1: 0.8798\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.89192\n",
      "Epoch 16/20\n",
      "48256/48256 [==============================] - 37s 763us/step - loss: 0.0805 - acc: 0.9747 - f1: 0.9749 - val_loss: 0.5801 - val_acc: 0.8874 - val_f1: 0.8876\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.89192\n",
      "Epoch 17/20\n",
      "48256/48256 [==============================] - 37s 765us/step - loss: 0.0749 - acc: 0.9775 - f1: 0.9776 - val_loss: 0.6800 - val_acc: 0.8880 - val_f1: 0.8887\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.89192\n",
      "Epoch 18/20\n",
      "48256/48256 [==============================] - 37s 763us/step - loss: 0.0675 - acc: 0.9802 - f1: 0.9800 - val_loss: 0.7416 - val_acc: 0.8803 - val_f1: 0.8801\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.89192\n",
      "Epoch 19/20\n",
      "48256/48256 [==============================] - 37s 763us/step - loss: 0.0637 - acc: 0.9821 - f1: 0.9820 - val_loss: 0.6880 - val_acc: 0.8896 - val_f1: 0.8900\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.89192\n",
      "Epoch 20/20\n",
      "48256/48256 [==============================] - 37s 762us/step - loss: 0.0553 - acc: 0.9842 - f1: 0.9842 - val_loss: 0.7222 - val_acc: 0.8879 - val_f1: 0.8886\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.89192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bbc3597f98>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train_new, x_train_24h, x_train_3m], y_train,\n",
    "           validation_data=([x_val_new, x_val_24h, x_val_3m], y_val),\n",
    "           callbacks=[checkpoint],\n",
    "           epochs=20,\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12065/12065 [==============================] - 1s 56us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7222323112859798, 0.887940323323172, 0.88859429980925]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score8 = model.evaluate([x_val_new, x_val_24h, x_val_3m], y_val, batch_size=256, verbose=1)\n",
    "score8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new, x_val_new, x_train_24h, x_train_3m, x_val_24h, x_val_3m, y_train, y_val = shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(SEQ_LEN,), dtype='int32')\n",
    "e = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=SEQ_LEN, trainable=False)(input_tensor)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(e)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(x)\n",
    "x_2 = Input(shape=(1,), dtype='float32')\n",
    "x_3 = Input(shape=(1,), dtype='float32')\n",
    "output = concatenate([x, x_2, x_3])\n",
    "output = Dense(256)(output)\n",
    "output = LeakyReLU(alpha=0.1)(output)\n",
    "output = Dropout(0.6)(output)\n",
    "output = Dense(128)(output)\n",
    "output = LeakyReLU(alpha=0.1)(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(64)(output)\n",
    "output = LeakyReLU(alpha=0.1)(output)\n",
    "output = Dropout(0.4)(output)\n",
    "output = Dense(7, activation='softmax')(output)\n",
    "model = Model([input_tensor, x_2, x_3], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 60, 300)      6243000     input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 60, 256)      440320      embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 128)          164864      bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 130)          0           bidirectional_20[0][0]           \n",
      "                                                                 input_26[0][0]                   \n",
      "                                                                 input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 256)          33536       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 256)          0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 128)          32896       dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 128)          0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 128)          0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 64)           8256        dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 64)           0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 64)           0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 7)            455         dropout_17[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,923,327\n",
      "Trainable params: 680,327\n",
      "Non-trainable params: 6,243,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('D:/Datasets/hackerearth/models/word2vec-lstm-6-model9.hdf5', monitor='val_acc', save_best_only=True, verbose=1, mode='max')\n",
    "model.compile(optimizer=Adam(lr=1e-3, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy', f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48256 samples, validate on 12065 samples\n",
      "Epoch 1/20\n",
      "48256/48256 [==============================] - 39s 816us/step - loss: 0.6858 - acc: 0.7728 - f1: 0.7541 - val_loss: 0.4269 - val_acc: 0.8528 - val_f1: 0.8463\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85280, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model9.hdf5\n",
      "Epoch 2/20\n",
      "48256/48256 [==============================] - 37s 774us/step - loss: 0.4542 - acc: 0.8500 - f1: 0.8458 - val_loss: 0.3849 - val_acc: 0.8684 - val_f1: 0.8563\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.85280 to 0.86838, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model9.hdf5\n",
      "Epoch 3/20\n",
      "48256/48256 [==============================] - 37s 776us/step - loss: 0.3884 - acc: 0.8676 - f1: 0.8649 - val_loss: 0.3394 - val_acc: 0.8782 - val_f1: 0.8789\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.86838 to 0.87824, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model9.hdf5\n",
      "Epoch 4/20\n",
      "48256/48256 [==============================] - 37s 775us/step - loss: 0.3410 - acc: 0.8815 - f1: 0.8793 - val_loss: 0.3240 - val_acc: 0.8840 - val_f1: 0.8837\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.87824 to 0.88396, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model9.hdf5\n",
      "Epoch 5/20\n",
      "48256/48256 [==============================] - 37s 772us/step - loss: 0.3088 - acc: 0.8925 - f1: 0.8910 - val_loss: 0.3254 - val_acc: 0.8846 - val_f1: 0.8858\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.88396 to 0.88462, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model9.hdf5\n",
      "Epoch 6/20\n",
      "48256/48256 [==============================] - 37s 776us/step - loss: 0.2769 - acc: 0.9008 - f1: 0.8993 - val_loss: 0.3201 - val_acc: 0.8894 - val_f1: 0.8893\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.88462 to 0.88943, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model9.hdf5\n",
      "Epoch 7/20\n",
      "48256/48256 [==============================] - 37s 775us/step - loss: 0.2465 - acc: 0.9121 - f1: 0.9114 - val_loss: 0.3326 - val_acc: 0.8837 - val_f1: 0.8853\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.88943\n",
      "Epoch 8/20\n",
      "48256/48256 [==============================] - 37s 773us/step - loss: 0.2185 - acc: 0.9207 - f1: 0.9201 - val_loss: 0.3211 - val_acc: 0.8976 - val_f1: 0.8991\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.88943 to 0.89764, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model9.hdf5\n",
      "Epoch 9/20\n",
      "48256/48256 [==============================] - 37s 776us/step - loss: 0.1916 - acc: 0.9314 - f1: 0.9316 - val_loss: 0.3484 - val_acc: 0.8965 - val_f1: 0.8968\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.89764\n",
      "Epoch 10/20\n",
      "48256/48256 [==============================] - 37s 774us/step - loss: 0.1620 - acc: 0.9427 - f1: 0.9426 - val_loss: 0.3716 - val_acc: 0.8893 - val_f1: 0.8894\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.89764\n",
      "Epoch 11/20\n",
      "48256/48256 [==============================] - 37s 775us/step - loss: 0.1365 - acc: 0.9520 - f1: 0.9515 - val_loss: 0.4982 - val_acc: 0.8871 - val_f1: 0.8877\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.89764\n",
      "Epoch 12/20\n",
      "48256/48256 [==============================] - 37s 775us/step - loss: 0.1224 - acc: 0.9584 - f1: 0.9586 - val_loss: 0.4591 - val_acc: 0.8952 - val_f1: 0.8958\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.89764\n",
      "Epoch 13/20\n",
      "48256/48256 [==============================] - 37s 774us/step - loss: 0.1018 - acc: 0.9652 - f1: 0.9651 - val_loss: 0.4366 - val_acc: 0.8952 - val_f1: 0.8962\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.89764\n",
      "Epoch 14/20\n",
      "48256/48256 [==============================] - 37s 772us/step - loss: 0.0855 - acc: 0.9714 - f1: 0.9714 - val_loss: 0.5199 - val_acc: 0.8943 - val_f1: 0.8950\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.89764\n",
      "Epoch 15/20\n",
      "48256/48256 [==============================] - 37s 772us/step - loss: 0.0753 - acc: 0.9758 - f1: 0.9760 - val_loss: 0.5720 - val_acc: 0.9003 - val_f1: 0.9010\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.89764 to 0.90029, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model9.hdf5\n",
      "Epoch 16/20\n",
      "48256/48256 [==============================] - 38s 777us/step - loss: 0.0659 - acc: 0.9790 - f1: 0.9789 - val_loss: 0.5431 - val_acc: 0.8926 - val_f1: 0.8939\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.90029\n",
      "Epoch 17/20\n",
      "48256/48256 [==============================] - 37s 772us/step - loss: 0.0539 - acc: 0.9830 - f1: 0.9831 - val_loss: 0.7124 - val_acc: 0.8963 - val_f1: 0.8967\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.90029\n",
      "Epoch 18/20\n",
      "48256/48256 [==============================] - 37s 775us/step - loss: 0.0562 - acc: 0.9827 - f1: 0.9828 - val_loss: 0.6268 - val_acc: 0.8973 - val_f1: 0.8979\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.90029\n",
      "Epoch 19/20\n",
      "48256/48256 [==============================] - 37s 776us/step - loss: 0.0525 - acc: 0.9843 - f1: 0.9842 - val_loss: 0.6088 - val_acc: 0.8970 - val_f1: 0.8971\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.90029\n",
      "Epoch 20/20\n",
      "48256/48256 [==============================] - 37s 773us/step - loss: 0.0449 - acc: 0.9871 - f1: 0.9871 - val_loss: 0.6636 - val_acc: 0.8938 - val_f1: 0.8946\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.90029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bbd45b55c0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train_new, x_train_24h, x_train_3m], y_train,\n",
    "           validation_data=([x_val_new, x_val_24h, x_val_3m], y_val),\n",
    "           callbacks=[checkpoint],\n",
    "           epochs=20,\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12065/12065 [==============================] - 1s 56us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6636065094193333, 0.8938251140401218, 0.8946281663026743]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score9 = model.evaluate([x_val_new, x_val_24h, x_val_3m], y_val, batch_size=256, verbose=1)\n",
    "score9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new, x_val_new, x_train_24h, x_train_3m, x_val_24h, x_val_3m, y_train, y_val = shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(SEQ_LEN,), dtype='int32')\n",
    "e = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=SEQ_LEN, trainable=False)(input_tensor)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(e)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(x)\n",
    "x_2 = Input(shape=(1,), dtype='float32')\n",
    "x_3 = Input(shape=(1,), dtype='float32')\n",
    "output = concatenate([x, x_2, x_3])\n",
    "output = Dense(256)(output)\n",
    "output = LeakyReLU(alpha=0.1)(output)\n",
    "output = Dropout(0.6)(output)\n",
    "output = Dense(128)(output)\n",
    "output = LeakyReLU(alpha=0.1)(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(64)(output)\n",
    "output = LeakyReLU(alpha=0.1)(output)\n",
    "output = Dropout(0.4)(output)\n",
    "output = Dense(7, activation='softmax')(output)\n",
    "model = Model([input_tensor, x_2, x_3], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_28 (InputLayer)           (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 60, 300)      6243000     input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 60, 256)      440320      embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional (None, 128)          164864      bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_29 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 130)          0           bidirectional_22[0][0]           \n",
      "                                                                 input_29[0][0]                   \n",
      "                                                                 input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 256)          33536       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 256)          0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 256)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 128)          32896       dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 128)          0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 128)          0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 64)           8256        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 64)           0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 64)           0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 7)            455         dropout_20[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,923,327\n",
      "Trainable params: 680,327\n",
      "Non-trainable params: 6,243,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('D:/Datasets/hackerearth/models/word2vec-lstm-6-model10.hdf5', monitor='val_acc', save_best_only=True, verbose=1, mode='max')\n",
    "model.compile(optimizer=Adam(lr=1e-3, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy', f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48256 samples, validate on 12065 samples\n",
      "Epoch 1/20\n",
      "48256/48256 [==============================] - 40s 819us/step - loss: 0.6865 - acc: 0.7751 - f1: 0.7582 - val_loss: 0.4344 - val_acc: 0.8556 - val_f1: 0.8422\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85562, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model10.hdf5\n",
      "Epoch 2/20\n",
      "48256/48256 [==============================] - 37s 777us/step - loss: 0.4438 - acc: 0.8540 - f1: 0.8488 - val_loss: 0.3895 - val_acc: 0.8628 - val_f1: 0.8572\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.85562 to 0.86283, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model10.hdf5\n",
      "Epoch 3/20\n",
      "48256/48256 [==============================] - 37s 776us/step - loss: 0.3851 - acc: 0.8699 - f1: 0.8664 - val_loss: 0.3732 - val_acc: 0.8682 - val_f1: 0.8650\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.86283 to 0.86821, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model10.hdf5\n",
      "Epoch 4/20\n",
      "48256/48256 [==============================] - 37s 777us/step - loss: 0.3415 - acc: 0.8839 - f1: 0.8813 - val_loss: 0.3770 - val_acc: 0.8670 - val_f1: 0.8667\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.86821\n",
      "Epoch 5/20\n",
      "48256/48256 [==============================] - 38s 778us/step - loss: 0.3090 - acc: 0.8937 - f1: 0.8912 - val_loss: 0.3497 - val_acc: 0.8782 - val_f1: 0.8793\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.86821 to 0.87816, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model10.hdf5\n",
      "Epoch 6/20\n",
      "48256/48256 [==============================] - 37s 775us/step - loss: 0.2770 - acc: 0.9036 - f1: 0.9024 - val_loss: 0.3132 - val_acc: 0.8929 - val_f1: 0.8927\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.87816 to 0.89291, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model10.hdf5\n",
      "Epoch 7/20\n",
      "48256/48256 [==============================] - 37s 776us/step - loss: 0.2468 - acc: 0.9134 - f1: 0.9128 - val_loss: 0.3333 - val_acc: 0.8897 - val_f1: 0.8912\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.89291\n",
      "Epoch 8/20\n",
      "48256/48256 [==============================] - 38s 780us/step - loss: 0.2148 - acc: 0.9247 - f1: 0.9239 - val_loss: 0.3367 - val_acc: 0.8914 - val_f1: 0.8923\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.89291\n",
      "Epoch 9/20\n",
      "48256/48256 [==============================] - 37s 776us/step - loss: 0.1859 - acc: 0.9332 - f1: 0.9332 - val_loss: 0.3404 - val_acc: 0.8932 - val_f1: 0.8952\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.89291 to 0.89324, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model10.hdf5\n",
      "Epoch 10/20\n",
      "48256/48256 [==============================] - 37s 776us/step - loss: 0.1622 - acc: 0.9438 - f1: 0.9439 - val_loss: 0.4025 - val_acc: 0.8908 - val_f1: 0.8922\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.89324\n",
      "Epoch 11/20\n",
      "48256/48256 [==============================] - 37s 774us/step - loss: 0.1346 - acc: 0.9536 - f1: 0.9537 - val_loss: 0.4536 - val_acc: 0.8872 - val_f1: 0.8881\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.89324\n",
      "Epoch 12/20\n",
      "48256/48256 [==============================] - 38s 779us/step - loss: 0.1158 - acc: 0.9604 - f1: 0.9607 - val_loss: 0.4568 - val_acc: 0.8947 - val_f1: 0.8946\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.89324 to 0.89474, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model10.hdf5\n",
      "Epoch 13/20\n",
      "48256/48256 [==============================] - 37s 776us/step - loss: 0.0971 - acc: 0.9677 - f1: 0.9678 - val_loss: 0.5003 - val_acc: 0.8919 - val_f1: 0.8926\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.89474\n",
      "Epoch 14/20\n",
      "48256/48256 [==============================] - 38s 778us/step - loss: 0.0826 - acc: 0.9732 - f1: 0.9730 - val_loss: 0.5770 - val_acc: 0.8933 - val_f1: 0.8932\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.89474\n",
      "Epoch 15/20\n",
      "48256/48256 [==============================] - 38s 778us/step - loss: 0.0730 - acc: 0.9769 - f1: 0.9770 - val_loss: 0.4911 - val_acc: 0.8922 - val_f1: 0.8924\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.89474\n",
      "Epoch 16/20\n",
      "48256/48256 [==============================] - 37s 776us/step - loss: 0.0635 - acc: 0.9798 - f1: 0.9800 - val_loss: 0.6309 - val_acc: 0.8901 - val_f1: 0.8909\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.89474\n",
      "Epoch 17/20\n",
      "48256/48256 [==============================] - 37s 775us/step - loss: 0.0528 - acc: 0.9835 - f1: 0.9837 - val_loss: 0.6986 - val_acc: 0.8916 - val_f1: 0.8913\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.89474\n",
      "Epoch 18/20\n",
      "48256/48256 [==============================] - 38s 787us/step - loss: 0.0551 - acc: 0.9837 - f1: 0.9837 - val_loss: 0.7229 - val_acc: 0.8956 - val_f1: 0.8955\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.89474 to 0.89557, saving model to D:/Datasets/hackerearth/models/word2vec-lstm-6-model10.hdf5\n",
      "Epoch 19/20\n",
      "48256/48256 [==============================] - 37s 775us/step - loss: 0.0445 - acc: 0.9865 - f1: 0.9864 - val_loss: 0.6199 - val_acc: 0.8902 - val_f1: 0.8908\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.89557\n",
      "Epoch 20/20\n",
      "48256/48256 [==============================] - 37s 772us/step - loss: 0.0463 - acc: 0.9873 - f1: 0.9871 - val_loss: 0.6693 - val_acc: 0.8907 - val_f1: 0.8915\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.89557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bbd934f390>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train_new, x_train_24h, x_train_3m], y_train,\n",
    "           validation_data=([x_val_new, x_val_24h, x_val_3m], y_val),\n",
    "           callbacks=[checkpoint],\n",
    "           epochs=20,\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12065/12065 [==============================] - 1s 56us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6692936087366066, 0.8906755077409092, 0.8915371985243803]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score10 = model.evaluate([x_val_new, x_val_24h, x_val_3m], y_val, batch_size=256, verbose=1)\n",
    "score10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>cleaned_hm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88305</td>\n",
       "      <td>3m</td>\n",
       "      <td>I spent the weekend in Chicago with my friends.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88306</td>\n",
       "      <td>3m</td>\n",
       "      <td>We moved back into our house after a remodel. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88307</td>\n",
       "      <td>3m</td>\n",
       "      <td>My fiance proposed to me in front of my family...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88308</td>\n",
       "      <td>3m</td>\n",
       "      <td>I ate lobster at a fancy restaurant with some ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88309</td>\n",
       "      <td>3m</td>\n",
       "      <td>I went out to a nice restaurant on a date with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid reflection_period                                         cleaned_hm\n",
       "0  88305                3m    I spent the weekend in Chicago with my friends.\n",
       "1  88306                3m  We moved back into our house after a remodel. ...\n",
       "2  88307                3m  My fiance proposed to me in front of my family...\n",
       "3  88308                3m  I ate lobster at a fancy restaurant with some ...\n",
       "4  88309                3m  I went out to a nice restaurant on a date with..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('D:/Datasets/hackerearth/hm_test.csv')\n",
    "df_test.drop(['num_sentence'], axis=1, inplace=True)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>cleaned_hm2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88305</td>\n",
       "      <td>3m</td>\n",
       "      <td>i spent the weekend in chicago with my friends .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88306</td>\n",
       "      <td>3m</td>\n",
       "      <td>we moved back into our house after a remodel ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88307</td>\n",
       "      <td>3m</td>\n",
       "      <td>my fiance proposed to me in front of my family...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88308</td>\n",
       "      <td>3m</td>\n",
       "      <td>i ate lobster at a fancy restaurant with some ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88309</td>\n",
       "      <td>3m</td>\n",
       "      <td>went nice restaurant date wife . popular resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid reflection_period                                        cleaned_hm2\n",
       "0  88305                3m  i spent the weekend in chicago with my friends . \n",
       "1  88306                3m  we moved back into our house after a remodel ....\n",
       "2  88307                3m  my fiance proposed to me in front of my family...\n",
       "3  88308                3m  i ate lobster at a fancy restaurant with some ...\n",
       "4  88309                3m  went nice restaurant date wife . popular resta..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.cleaned_hm = df_test.cleaned_hm.str.lower()\n",
    "df_test['cleaned_hm2'] = df_test.cleaned_hm.apply(remove_stopwords)\n",
    "df_test.drop(['cleaned_hm'], axis=1, inplace=True)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.205356476761246"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_set = t.texts_to_sequences(df_test.cleaned_hm2)\n",
    "lengths = []\n",
    "for doc in encoded_test_set:\n",
    "    lengths.append(len(doc))\n",
    "    \n",
    "np.mean(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>cleaned_hm2</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88305</td>\n",
       "      <td>3m</td>\n",
       "      <td>i spent the weekend in chicago with my friends .</td>\n",
       "      <td>[1, 198, 6, 172, 10, 1928, 11, 2, 48]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88306</td>\n",
       "      <td>3m</td>\n",
       "      <td>we moved back into our house after a remodel ....</td>\n",
       "      <td>[25, 416, 96, 160, 58, 107, 44, 3, 5123, 25, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88307</td>\n",
       "      <td>3m</td>\n",
       "      <td>my fiance proposed to me in front of my family...</td>\n",
       "      <td>[2, 682, 1762, 4, 9, 10, 562, 13, 2, 49, 10, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88308</td>\n",
       "      <td>3m</td>\n",
       "      <td>i ate lobster at a fancy restaurant with some ...</td>\n",
       "      <td>[1, 158, 4647, 20, 3, 1517, 239, 11, 46, 48]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88309</td>\n",
       "      <td>3m</td>\n",
       "      <td>went nice restaurant date wife . popular resta...</td>\n",
       "      <td>[21, 81, 239, 314, 88, 1710, 239, 151, 55, 511...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid reflection_period                                        cleaned_hm2  \\\n",
       "0  88305                3m  i spent the weekend in chicago with my friends .    \n",
       "1  88306                3m  we moved back into our house after a remodel ....   \n",
       "2  88307                3m  my fiance proposed to me in front of my family...   \n",
       "3  88308                3m  i ate lobster at a fancy restaurant with some ...   \n",
       "4  88309                3m  went nice restaurant date wife . popular resta...   \n",
       "\n",
       "                                              tokens  \n",
       "0              [1, 198, 6, 172, 10, 1928, 11, 2, 48]  \n",
       "1  [25, 416, 96, 160, 58, 107, 44, 3, 5123, 25, 1...  \n",
       "2  [2, 682, 1762, 4, 9, 10, 562, 13, 2, 49, 10, 6...  \n",
       "3       [1, 158, 4647, 20, 3, 1517, 239, 11, 46, 48]  \n",
       "4  [21, 81, 239, 314, 88, 1710, 239, 151, 55, 511...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['tokens'] = encoded_test_set\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88305</td>\n",
       "      <td>3m</td>\n",
       "      <td>[1, 198, 6, 172, 10, 1928, 11, 2, 48]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88306</td>\n",
       "      <td>3m</td>\n",
       "      <td>[25, 416, 96, 160, 58, 107, 44, 3, 5123, 25, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88307</td>\n",
       "      <td>3m</td>\n",
       "      <td>[2, 682, 1762, 4, 9, 10, 562, 13, 2, 49, 10, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88308</td>\n",
       "      <td>3m</td>\n",
       "      <td>[1, 158, 4647, 20, 3, 1517, 239, 11, 46, 48]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88309</td>\n",
       "      <td>3m</td>\n",
       "      <td>[21, 81, 239, 314, 88, 1710, 239, 151, 55, 511...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid reflection_period                                             tokens\n",
       "0  88305                3m              [1, 198, 6, 172, 10, 1928, 11, 2, 48]\n",
       "1  88306                3m  [25, 416, 96, 160, 58, 107, 44, 3, 5123, 25, 1...\n",
       "2  88307                3m  [2, 682, 1762, 4, 9, 10, 562, 13, 2, 49, 10, 6...\n",
       "3  88308                3m       [1, 158, 4647, 20, 3, 1517, 239, 11, 46, 48]\n",
       "4  88309                3m  [21, 81, 239, 314, 88, 1710, 239, 151, 55, 511..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.drop(['cleaned_hm2'], axis=1, inplace=True)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_test_set = pad_sequences(encoded_test_set, maxlen=SEQ_LEN, padding='post')\n",
    "paddocs_test = []\n",
    "for doc in padded_test_set:\n",
    "    paddocs_test.append(list(doc))\n",
    "    \n",
    "df_test['tokens2'] = paddocs_test\n",
    "lengths = []\n",
    "for doc in paddocs_test:\n",
    "    lengths.append(len(doc))\n",
    "np.mean(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88305</td>\n",
       "      <td>3m</td>\n",
       "      <td>[1, 198, 6, 172, 10, 1928, 11, 2, 48, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88306</td>\n",
       "      <td>3m</td>\n",
       "      <td>[25, 416, 96, 160, 58, 107, 44, 3, 5123, 25, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88307</td>\n",
       "      <td>3m</td>\n",
       "      <td>[2, 682, 1762, 4, 9, 10, 562, 13, 2, 49, 10, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88308</td>\n",
       "      <td>3m</td>\n",
       "      <td>[1, 158, 4647, 20, 3, 1517, 239, 11, 46, 48, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88309</td>\n",
       "      <td>3m</td>\n",
       "      <td>[21, 81, 239, 314, 88, 1710, 239, 151, 55, 511...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid reflection_period                                             tokens\n",
       "0  88305                3m  [1, 198, 6, 172, 10, 1928, 11, 2, 48, 0, 0, 0,...\n",
       "1  88306                3m  [25, 416, 96, 160, 58, 107, 44, 3, 5123, 25, 1...\n",
       "2  88307                3m  [2, 682, 1762, 4, 9, 10, 562, 13, 2, 49, 10, 6...\n",
       "3  88308                3m  [1, 158, 4647, 20, 3, 1517, 239, 11, 46, 48, 0...\n",
       "4  88309                3m  [21, 81, 239, 314, 88, 1710, 239, 151, 55, 511..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.drop(['tokens'], axis=1, inplace=True)\n",
    "df_test.rename(index=str, columns={'tokens2': 'tokens'}, inplace=True)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>24h</th>\n",
       "      <th>3m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88305</td>\n",
       "      <td>[1, 198, 6, 172, 10, 1928, 11, 2, 48, 0, 0, 0,...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88306</td>\n",
       "      <td>[25, 416, 96, 160, 58, 107, 44, 3, 5123, 25, 1...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88307</td>\n",
       "      <td>[2, 682, 1762, 4, 9, 10, 562, 13, 2, 49, 10, 6...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88308</td>\n",
       "      <td>[1, 158, 4647, 20, 3, 1517, 239, 11, 46, 48, 0...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88309</td>\n",
       "      <td>[21, 81, 239, 314, 88, 1710, 239, 151, 55, 511...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid                                             tokens    24h    3m\n",
       "0  88305  [1, 198, 6, 172, 10, 1928, 11, 2, 48, 0, 0, 0,...  False  True\n",
       "1  88306  [25, 416, 96, 160, 58, 107, 44, 3, 5123, 25, 1...  False  True\n",
       "2  88307  [2, 682, 1762, 4, 9, 10, 562, 13, 2, 49, 10, 6...  False  True\n",
       "3  88308  [1, 158, 4647, 20, 3, 1517, 239, 11, 46, 48, 0...  False  True\n",
       "4  88309  [21, 81, 239, 314, 88, 1710, 239, 151, 55, 511...  False  True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['24h'] = df_test.reflection_period == '24h'\n",
    "df_test['3m'] = df_test.reflection_period == '3m'\n",
    "df_test.drop(['reflection_period'], axis=1, inplace=True)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40213, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test.tokens\n",
    "x_test_new = []\n",
    "for element in x_test:\n",
    "    x_test_new.append(np.array(element))\n",
    "x_test_new = np.array(x_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_24h = df_test['24h']\n",
    "x_test_3m = df_test['3m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('D:/Datasets/hackerearth/models/word2vec-lstm-6-model1.hdf5', custom_objects={'f1': f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40213/40213 [==============================] - 6s 144us/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([x_test_new, x_test_24h, x_test_3m], batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('D:/Datasets/hackerearth/models/word2vec-lstm-6-model1-preds.npy', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('D:/Datasets/hackerearth/models/word2vec-lstm-6-model2.hdf5', custom_objects={'f1': f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40213/40213 [==============================] - 4s 108us/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([x_test_new, x_test_24h, x_test_3m], batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('D:/Datasets/hackerearth/models/word2vec-lstm-6-model2-preds.npy', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40213/40213 [==============================] - 2s 60us/step\n"
     ]
    }
   ],
   "source": [
    "model = load_model('D:/Datasets/hackerearth/models/word2vec-lstm-6-model3.hdf5', custom_objects={'f1': f1})\n",
    "preds = model.predict([x_test_new, x_test_24h, x_test_3m], batch_size=256, verbose=1)\n",
    "np.save('D:/Datasets/hackerearth/models/word2vec-lstm-6-model3-preds.npy', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40213/40213 [==============================] - 2s 60us/step\n"
     ]
    }
   ],
   "source": [
    "model = load_model('D:/Datasets/hackerearth/models/word2vec-lstm-6-model4.hdf5', custom_objects={'f1': f1})\n",
    "preds = model.predict([x_test_new, x_test_24h, x_test_3m], batch_size=256, verbose=1)\n",
    "np.save('D:/Datasets/hackerearth/models/word2vec-lstm-6-model4-preds.npy', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40213/40213 [==============================] - 3s 66us/step\n"
     ]
    }
   ],
   "source": [
    "model = load_model('D:/Datasets/hackerearth/models/word2vec-lstm-6-model5.hdf5', custom_objects={'f1': f1})\n",
    "preds = model.predict([x_test_new, x_test_24h, x_test_3m], batch_size=256, verbose=1)\n",
    "np.save('D:/Datasets/hackerearth/models/word2vec-lstm-6-model5-preds.npy', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40213/40213 [==============================] - 3s 68us/step\n"
     ]
    }
   ],
   "source": [
    "model = load_model('D:/Datasets/hackerearth/models/word2vec-lstm-6-model6.hdf5', custom_objects={'f1': f1})\n",
    "preds = model.predict([x_test_new, x_test_24h, x_test_3m], batch_size=256, verbose=1)\n",
    "np.save('D:/Datasets/hackerearth/models/word2vec-lstm-6-model6-preds.npy', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40213/40213 [==============================] - 3s 64us/step\n"
     ]
    }
   ],
   "source": [
    "model = load_model('D:/Datasets/hackerearth/models/word2vec-lstm-6-model7.hdf5', custom_objects={'f1': f1})\n",
    "preds = model.predict([x_test_new, x_test_24h, x_test_3m], batch_size=256, verbose=1)\n",
    "np.save('D:/Datasets/hackerearth/models/word2vec-lstm-6-model7-preds.npy', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40213/40213 [==============================] - 3s 64us/step\n"
     ]
    }
   ],
   "source": [
    "model = load_model('D:/Datasets/hackerearth/models/word2vec-lstm-6-model8.hdf5', custom_objects={'f1': f1})\n",
    "preds = model.predict([x_test_new, x_test_24h, x_test_3m], batch_size=256, verbose=1)\n",
    "np.save('D:/Datasets/hackerearth/models/word2vec-lstm-6-model8-preds.npy', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40213/40213 [==============================] - 3s 66us/step\n"
     ]
    }
   ],
   "source": [
    "model = load_model('D:/Datasets/hackerearth/models/word2vec-lstm-6-model9.hdf5', custom_objects={'f1': f1})\n",
    "preds = model.predict([x_test_new, x_test_24h, x_test_3m], batch_size=256, verbose=1)\n",
    "np.save('D:/Datasets/hackerearth/models/word2vec-lstm-6-model9-preds.npy', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40213/40213 [==============================] - 3s 66us/step\n"
     ]
    }
   ],
   "source": [
    "model = load_model('D:/Datasets/hackerearth/models/word2vec-lstm-6-model10.hdf5', custom_objects={'f1': f1})\n",
    "preds = model.predict([x_test_new, x_test_24h, x_test_3m], batch_size=256, verbose=1)\n",
    "np.save('D:/Datasets/hackerearth/models/word2vec-lstm-6-model10-preds.npy', preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-trainable hard-voting\n",
    "Used to generate `submission_word2vec-lstm-6-hard.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in range(1, 11):\n",
    "    pred = np.load(f'D:/Datasets/hackerearth/models/word2vec-lstm-6-model{i}-preds.npy')\n",
    "    predictions.append(pred)\n",
    "    \n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 40213, 7)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = []\n",
    "\n",
    "for i in range(40213):\n",
    "    count = np.zeros((11,))\n",
    "    for j in range(10):\n",
    "        count[np.argmax(predictions[j][i])] += 1\n",
    "    \n",
    "    dummy = np.zeros((7,))\n",
    "    dummy[np.argmax(count)] = 1\n",
    "    votes.append(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "for vote in votes:\n",
    "    categories.append(cats_to_labels[tuple(vote)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bonding',\n",
       " 'achievement',\n",
       " 'affection',\n",
       " 'bonding',\n",
       " 'affection',\n",
       " 'leisure',\n",
       " 'achievement',\n",
       " 'affection',\n",
       " 'leisure',\n",
       " 'bonding']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>24h</th>\n",
       "      <th>3m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88305</td>\n",
       "      <td>[1, 198, 6, 172, 10, 1928, 11, 2, 48, 0, 0, 0,...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88306</td>\n",
       "      <td>[25, 416, 96, 160, 58, 107, 44, 3, 5123, 25, 1...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88307</td>\n",
       "      <td>[2, 682, 1762, 4, 9, 10, 562, 13, 2, 49, 10, 6...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88308</td>\n",
       "      <td>[1, 158, 4647, 20, 3, 1517, 239, 11, 46, 48, 0...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88309</td>\n",
       "      <td>[21, 81, 239, 314, 88, 1710, 239, 151, 55, 511...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid                                             tokens    24h    3m\n",
       "0  88305  [1, 198, 6, 172, 10, 1928, 11, 2, 48, 0, 0, 0,...  False  True\n",
       "1  88306  [25, 416, 96, 160, 58, 107, 44, 3, 5123, 25, 1...  False  True\n",
       "2  88307  [2, 682, 1762, 4, 9, 10, 562, 13, 2, 49, 10, 6...  False  True\n",
       "3  88308  [1, 158, 4647, 20, 3, 1517, 239, 11, 46, 48, 0...  False  True\n",
       "4  88309  [21, 81, 239, 314, 88, 1710, 239, 151, 55, 511...  False  True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid\n",
       "0  88305\n",
       "1  88306\n",
       "2  88307\n",
       "3  88308\n",
       "4  88309"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = df_test.copy()\n",
    "df_pred.drop(['tokens', '24h', '3m'], axis=1, inplace=True)\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40213"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred['predicted_category'] = categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.to_csv('D:/Datasets/hackerearth/submission_word2vec-lstm-6-hard.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-trainable soft voting\n",
    "Used to generate `submission_word2vec-lstm-6-soft.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in range(1, 11):\n",
    "    pred = np.load(f'D:/Datasets/hackerearth/models/word2vec-lstm-6-model{i}-preds.npy')\n",
    "    predictions.append(pred)\n",
    "    \n",
    "predictions = np.array(predictions)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 40213, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = []\n",
    "\n",
    "for i in range(40213):\n",
    "    vote = np.zeros((7,))\n",
    "    for j in range(10):\n",
    "        vote += predictions[j][i]\n",
    "        \n",
    "    dummy = np.zeros((7,))\n",
    "    dummy[np.argmax(vote)] = 1\n",
    "    votes.append(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "for vote in votes:\n",
    "    categories.append(cats_to_labels[tuple(vote)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bonding',\n",
       " 'achievement',\n",
       " 'affection',\n",
       " 'bonding',\n",
       " 'affection',\n",
       " 'leisure',\n",
       " 'achievement',\n",
       " 'affection',\n",
       " 'leisure',\n",
       " 'bonding']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>cleaned_hm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88305</td>\n",
       "      <td>3m</td>\n",
       "      <td>I spent the weekend in Chicago with my friends.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88306</td>\n",
       "      <td>3m</td>\n",
       "      <td>We moved back into our house after a remodel. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88307</td>\n",
       "      <td>3m</td>\n",
       "      <td>My fiance proposed to me in front of my family...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88308</td>\n",
       "      <td>3m</td>\n",
       "      <td>I ate lobster at a fancy restaurant with some ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88309</td>\n",
       "      <td>3m</td>\n",
       "      <td>I went out to a nice restaurant on a date with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid reflection_period                                         cleaned_hm\n",
       "0  88305                3m    I spent the weekend in Chicago with my friends.\n",
       "1  88306                3m  We moved back into our house after a remodel. ...\n",
       "2  88307                3m  My fiance proposed to me in front of my family...\n",
       "3  88308                3m  I ate lobster at a fancy restaurant with some ...\n",
       "4  88309                3m  I went out to a nice restaurant on a date with..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('D:/Datasets/hackerearth/hm_test.csv')\n",
    "df_test.drop(['num_sentence'], axis=1, inplace=True)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(['reflection_period', 'cleaned_hm'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid\n",
       "0  88305\n",
       "1  88306\n",
       "2  88307\n",
       "3  88308\n",
       "4  88309"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['predicted_category'] = categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40213, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88305</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88306</td>\n",
       "      <td>achievement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88307</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88308</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88309</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid predicted_category\n",
       "0  88305            bonding\n",
       "1  88306        achievement\n",
       "2  88307          affection\n",
       "3  88308            bonding\n",
       "4  88309          affection"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('D:/Datasets/hackerearth/submission_word2vec-lstm-6-soft.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if both methods differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soft = pd.read_csv('D:/Datasets/hackerearth/submission_word2vec-lstm-6-soft.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hard = pd.read_csv('D:/Datasets/hackerearth/submission_word2vec-lstm-6-hard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(df_hard)):\n",
    "    if df_hard.predicted_category[i] == df_soft.predicted_category[i]:\n",
    "        count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39724"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.78397533136051"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count * 100 / len(df_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft voting and hard voting is 98% similar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
