First to get a feel of the data, I trained my own word2vector embeddings and got an accuracy of about 53% using only dense layers. To improve the accuracy,I used the pretrained google news word2vector embeddings and got an accuracy of about 65% using the same neural network architecture. Then I implemented LSTM on keras and got an accuracy of about 72% as LSTM was a good option for sentiment analysis as it was taking the context of the sentence.After further tuning the hyperparameters the highest accuracy that I could achieve on the validation set was about 78%. On changing the LSTM layer to a bidirectional one, the accuracy  improved to around 86% on the validation set. The maximum accuracy that I could attain after further tuning the hyperparameters was around 88%. On using the LeakyRelu instead of ReLu as the activation function the accuracy improved to 88.5% on the validation set. 

I had also experimented with the stop words as to whether to remove them completely or remove them partially. On removing the stop words completely the validation accuracy decreased by about 3% on average on all the models tested.On removing them completely I deduced that I had also removed the context of the statement which led to the decrease in the validation accuracy. Also on further analysing the dataset I noticed that the data was in British English format and the pretrained Google word2vec embeddings was trained on American English so I converted some of the words that I could thinkk of like 'Colour' to color and many more as mentioned the function.

On using Reflection Period as one of the features in the neural architecture the validation accuracy increased by average of 1.5% on all of the models.

I had also tried training the model with the ELMO embeddings but the process was computationally very expensive and after training it for 5 epochs I got an validation accuracy of about 87%, this model has a very good chance of improvement as the training accuracy was also around 89%. Therefore the model was not overfitting in any way so training it for more than 20 epochs would surely have fetched us a validation accuracy above 90% but I could not do so duw to the hardware limitations. 

The model submitted was an ensemble of two instances each of the 5 best architectures that I had experimented with. To take in consideration the whole training data every model was retrained on the different split of the training data thus reducing the correlation among the models nad randomising the input data to each of the models. The accuracy of the ensemble model was around 90%.

